{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS UC Berkeley - Machine Learning at Scale\n",
    "## DATSCIW261 ASSIGNMENT #1  \n",
    "\n",
    "[James Gray](https://github.com/jamesgray007)   \n",
    "jamesgray@ischool.berkeley.edu   \n",
    "Time of Initial Submission: 9:21 PM EST, Monday, January 18, 2016  \n",
    "Time of **Resubmission**: 8:38 AM EST, Friday, January 22, 2016 \n",
    "W261-3, Spring 2016  \n",
    "Week 1 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References for this Assignment\n",
    "\n",
    "- **[Original Assignment Instructions](https://www.dropbox.com/sh/jylzkmauxkostck/AAA2pH0cTvb0zDrbbbze3zf-a/hw1_instructions.txt?dl=0)**\n",
    "- [Wikipedia explaination of Naive Bayes document classification](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification)\n",
    "- [Original paper describing the background of the Enron email corpus](http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf)\n",
    "- [Documentation for Scikit-Learn implementation of Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- [Stanford NLP Group's explaination of Naive Bayes algorithm](http://nlp.stanford.edu/IR-book/html/htmledition/properties-of-naive-bayes-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 0.0 Bio\n",
    "\n",
    "Prepare your bio and include it in this HW submission. Please limit to 100 words. Count the words in your bio and print the length of your bio (in terms of words) in a separate cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bio wordcount = 97\n"
     ]
    }
   ],
   "source": [
    "bio = \"Over the last five months I have worked as a data scientist in the Windows Engineering group at Microsoft \\\n",
    "focused on optimizing customer support experiences.  Prior to that I led a team responsible for delivering data \\\n",
    "platform and analytics across the sales, marketing, customer support and consulting services businesses of Microsoft.\\\n",
    "I relocated to Austin about a year ago after residing in Seattle for 15 years. I earned my MBA at Berkeley and \\\n",
    "BS in Electrical Engineering from Union College.  My goal for W261 is to deepen my practical machine learning \\\n",
    "experience both in theory and hands-on programming.\"\n",
    "\n",
    "wordcount = len(bio.split())\n",
    "print (\"Bio wordcount = \" + str(wordcount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.0.0. (Graded)\n",
    "\n",
    "*Define big data. Provide an example of a big data problem in your domain of expertise.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, Big Data is defined as data that are so large, complex and high velocity that traditional techniques are not adequate to process and deliver insights. Big Data is often described by three or four V's.  **Volume** often distinquishes Big Data as datasets that reach the petabyte (10^15) or zettbyte (10^21) scale when compared to traditional data processing applications in the terabytes (10^12). Data these large would not fit on a typical high performance laptop with 1TB disk storage. Processing and reading even 1TB data on a laptop would take approximately 3 hours which is not satisfactory given the need to delivery timely insights. **Variety** includes both structured and unstructed data such as video, text, JSON, images unlike traditional systems (relational databases) that handle well-formed data in rows and columns. **Velocity** includes the ability of Big Data solutions to handle very fast streaming data from sources such as the social web or machine-to-machine scenarios in the [Internet of Things](https://en.wikipedia.org/wiki/Internet_of_Things) realm. A typical laptop with even large memory would not be able to handle and persist the velocity of these data flows.  Traditional data processing systems such as a data warehouses operate in batch mode that generally refresh 1-2 times a day. **Veracity** is the uncertainty of data and ability to manage and enforce data quality.  All of these attributes make traditional data processing systems such as databases completely inadequate.\n",
    "\n",
    "![bigdata](img/bigdata.png)\n",
    "\n",
    "An example of a Big Data scenario at Microsoft is the collection of telemetry from hundreds of millions of Windows devices across the world on a daily basis. This results in high velocity of incoming data from user-driven actions (100's of millions of users) and application health monitoring events. A large distributed system is required to store and process the petabyte scale dataset that is collected each day.   \n",
    "\n",
    "References: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.0.1. (Graded)\n",
    "\n",
    "*In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1,2,3,4,5 are considered. How would you select a model?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Model selection is based on the model with the lowest overall error.  Error is composed of reproducible error (squared bias, variance) and irreproducible error that is inherent to the natural variability of the system. We can use regression to explain the model selection process based on error.  Error is composoed of three factors:\n",
    "\n",
    "*Expected prediction error = estimator variance + squared estimator bias + noise*\n",
    "\n",
    "* error due to variance = the error by which the prediction over one training set differs from the expected predictor overall training sets T\n",
    "* error due to squared bias = the error by which the expected model prediction (average) differs from the true value over the training data T.\n",
    "\n",
    "For a given estimator $g(x)$ and a true function $f(x)$, we represent error as follows:\n",
    "\n",
    "$$\n",
    "Err=(E[\\hat{y}]-y)^2+E[\\hat{y}-E[\\hat{y}]]^2+E[(y_{true}-y)^2]\n",
    "$$\n",
    "\n",
    "\n",
    "References used:\n",
    "* [Model Selection: Underfitting, Overfitting and the Bias-Variance Tradeoff](https://theclevermachine.wordpress.com/tag/bias-variance-tradeoff/)\n",
    "* [Ask a Data Scientist: The Bias vs. Variance Tradeoff](http://insidebigdata.com/2014/10/22/ask-data-scientist-bias-vs-variance-tradeoff/)\n",
    "* The elements of statistical learning: Data mining, inference and prediction (Chapter 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for Spam Filter using Naive Bayes Classifier\n",
    "\n",
    "In the remainder of this assignment you will produce a spam filter that is backed by a multinomial naive Bayes classifier b (see http://nlp.stanford.edu/IR-book/html/htmledition/properties-of-naive-bayes-1.html),\n",
    "which counts words in parallel via a unix, poor-man's map-reduce framework.  \n",
    "\n",
    "The data you will use is a curated subset of the Enron email corpus(whose details you may find in the file enronemail_README.txt in the directory surrounding these instructions).\n",
    "\n",
    "In this directory you will also find starter code (pNaiveBayes.sh),(similar to the pGrepCount.sh code that was presented in this weeks lectures), which will be used as control script to a python mapper and reducer that you will supply at several stages. Doing some exploratory data analysis you will see (with this very small dataset) the following\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis of Enron Email corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      99 enronemail_1h.txt\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l enronemail_1h.txt  # count the number of lines in the subset of enron emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     100     100     200\r\n"
     ]
    }
   ],
   "source": [
    "! cut -f2 -d$'\\t' enronemail_1h.txt|wc  #extract second field which is SPAM flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "0\r\n",
      "0\r\n",
      "0\r\n",
      "0\r\n",
      "0\r\n",
      "0\r\n",
      "0\r\n",
      "1\r\n",
      "1\r\n"
     ]
    }
   ],
   "source": [
    "! cut -f2 -d$'\\t' enronemail_1h.txt|head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b7\u001b[?47h\u001b[?1h\u001b=\r",
      "0018.2003-12-18.GP      1        await your response    \" dear partner,  we are  \ba team of government officials that belong to an eight-man committee in the pres \bidential cabinet as well as the senate.  at the moment, we will be requiring you \br assistance in a matter that involves investment of monies, which we intend to  \btransfer to your account, upon clarification and a workable agreement reached in \b consummating the project with you. based on a recommendation from an associate  \bconcerning your integrity, loyalty and understanding, we deemed it necessary to  \bcontact you accordingly. all arrangements in relation to this investment initiat \bive, as well as the initial capital for its take off has been tactically set asi \bde to commence whatever business you deemed fit, that will turn around profit fa \bvourably. we request you immediately contact us if you will be favorably dispose \bd to act as a partner in this venture, and possibly will afford us the opportuni \bty to discuss whatever proposal you may come up with. also  bear in mind that th \be initial capital that we shall send across will not exceed$ 13,731, 000,00 usd  \b(thirteen million seven hundred and thirty one thousand united states dollars) s \bo whatever areas of investment your proposal shall cover, please it should be wi \bthin the set aside capital. in this regard, the proposal you may wish to discuss \b with us should be comprehensive enough for our better understanding; with speci \bal emphasis on the following:  1. the tax obligationin your country  2. the init \bial capital base required in your proposed  investment area, as well as;  3. the \b legal technicalities in setting up a  business in your country with foreigners  \bas share-holders  4. the most convenient and secured mode of receiving the funds \b without our direct involvement.  5. your ability to provide a beneficiary/partn \b:\u001b[K"
     ]
    }
   ],
   "source": [
    "! head -n 100 enronemail_1h.txt|tail -1|less #an example SPAM email record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.1.\n",
    "\n",
    "Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. A simple cell in the notebook with a print statement with  a \"done\" string will suffice here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## pNaiveBayes.sh\n",
      "## Author: Jake Ryland Williams\n",
      "## Usage: pNaiveBayes.sh m wordlist\n",
      "## Input:\n",
      "##       m = number of processes (maps), e.g., 4\n",
      "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
      "##\n",
      "## Instructions: Read this script and its comments closely.\n",
      "##               Do your best to understand the purpose of each command,\n",
      "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
      "##               as this will determine how the python scripts take input.\n",
      "##               When you are comfortable with the unix code below,\n",
      "##               answer the questions on the LMS for HW1 about the starter code.\n",
      "\n",
      "## collect user input\n",
      "m=$1 ## the number of parallel processes (maps) to run\n",
      "wordlist=$2 ## if set to \"*\", then all words are used\n",
      "\n",
      "## a test set data of 100 messages\n",
      "data=\"enronemail_1h.txt\" \n",
      "\n",
      "## the full set of data (33746 messages)\n",
      "# data=\"enronemail.txt\" \n",
      "\n",
      "## 'wc' determines the number of lines in the data\n",
      "## 'perl -pe' regex strips the piped wc output to a number\n",
      "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
      "\n",
      "## determine the lines per chunk for the desired number of processes\n",
      "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
      "\n",
      "## split the original file into chunks by line\n",
      "split -l $linesinchunk $data $data.chunk.\n",
      "\n",
      "## assign python mappers (mapper.py) to the chunks of data\n",
      "## and emit their output to temporary files\n",
      "for datachunk in $data.chunk.*; do\n",
      "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
      "    ####\n",
      "    ####\n",
      "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
      "    ####\n",
      "    ####\n",
      "done\n",
      "## wait for the mappers to finish their work\n",
      "wait\n",
      "\n",
      "## 'ls' makes a list of the temporary count files\n",
      "## 'perl -pe' regex replaces line breaks with spaces\n",
      "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
      "\n",
      "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
      "####\n",
      "####\n",
      "./reducer.py $countfiles > $data.output\n",
      "####\n",
      "####\n",
      "\n",
      "## clean up the data chunks and temporary count files\n",
      "\\rm $data.chunk.*\n",
      "\n",
      "Question 1.1: DONE\n"
     ]
    }
   ],
   "source": [
    "#Display contents of pNaiveBayes.sh (it's convenient to keep everything in one notebook)\n",
    "!cat pNaiveBayes.sh\n",
    "!echo \"\"\n",
    "!echo \"Question 1.1: DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.2. \n",
    "\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.  To do so, make sure that:\n",
    "   \n",
    "* mapper.py counts all occurrences of a single word, and\n",
    "* reducer.py collates the counts of the single word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2 Mapper to process Enron email corpus (100 emails)\n",
    "\n",
    "All messages are collated to a tab-delimited format:\n",
    "\n",
    "ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
    "\n",
    "where:\n",
    "\n",
    "ID = string; unique message identifier  \n",
    "SPAM = binary; with 1 indicating a spam message   \n",
    "SUBJECT = string; title of the message   \n",
    "CONTENT = string; content of the message  \n",
    "\n",
    "Note that either of SUBJECT or CONTENT may be \"NA\",\n",
    "and that all tab (\\t) and newline (\\n) characters\n",
    "have been removed from both of the SUBJECT and CONTENT columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: James Gray\n",
    "## Description: mapper code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2].lower()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    # examine each line of the list of lines\n",
    "    for line in myfile.readlines( ):\n",
    "        text = \"\"\n",
    "        # parse each line to pull out the subject and body and concatenate into a new string\n",
    "        enronEmail = text.join(line.split('\\t')[-2:]) # take the last two words in the list (subject and body) \n",
    "        for word in WORD_RE.findall(enronEmail):\n",
    "            if word == findword:\n",
    "                count+=1\n",
    "print count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set file priveleges to execute script\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reducer to total the occurences of a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "sum = 0\n",
    "for file in sys.argv[1:]: # argument passed in is a list of chunk count files (enronemail_1h.txt.chunk_xx.counts)\n",
    "    # open each chunk count file\n",
    "    with open(file, \"r\") as chunkfilecount:\n",
    "        for line in chunkfilecount.readlines():\n",
    "            tokens=line.split('\\t') # the only token in this file is the count\n",
    "            sum+=int(tokens[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set file priveleges to execute script\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Execute Unix control script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set priveleges to execute Unix script\n",
    "!chmod a+x pNaiveBayes.sh\n",
    "\n",
    "# parameter #1 -> # of mappers to run\n",
    "# parameter #2 -> wordlist (the word(s) to search for and count)\n",
    "\n",
    "!./pNaiveBayes.sh 2 assistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 View Output and Cross check against Grep command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of occurences of \"assistance\" in the subject and body:\n",
      "10\n",
      "The number of occurences of \"assistance\" in body using grep:\n",
      "       8\n"
     ]
    }
   ],
   "source": [
    "# read the output file created from the Unix control script\n",
    "print (\"The number of occurences of \\\"assistance\\\" in the subject and body:\") \n",
    "!cat enronemail_1h.txt.output\n",
    "\n",
    "# check against grep\n",
    "print (\"The number of occurences of \\\"assistance\\\" in body using grep:\") \n",
    "!grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a slight descrepency between the Unix control script and Grep command since the Grep command only evaluates the body."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.3. (Graded)\n",
    "\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results.\n",
    "   \n",
    "To do so, make sure that:\n",
    "   \n",
    "   - mapper.py and\n",
    "   - reducer.py \n",
    "\n",
    "performs a single word Naive Bayes classification. \n",
    "\n",
    "For multinomial Naive Bayes, the $Pr(X=“assistance”|Y=SPAM)$ is calculated as follows:\n",
    "\n",
    "the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "NOTE if  “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then $Pr(X=“assistance”|Y=SPAM) = 5/1000$. Note this is a multinomial estimated of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW.\n",
    "\n",
    "The Naive Bayes classifier will predict the probability that an email is SPAM given the evidence of a predefined word (in this case the word \"assistance\").  The formula for Bayes theorem (from the book \"Making Sense of Data II\")\n",
    "\n",
    "![bayes](img/nbformula.gif)\n",
    "\n",
    "* P(H|E) = probability of a hypothesis(H) given some evidence(E) -> probability of SPAM given evidence of \"assistance\"\n",
    "* P(E|H) = probability of evidence(E) given some hypothesis(H) ->  probability of \"assistance\" given SPAM\n",
    "* P(H) = probability of the hypothesis (SPAM)\n",
    "* P(E) = probability of the evidence (the word \"assistance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.3 SPAM Classifier Mapper\n",
    "\n",
    "This mapper function will send one line for every instance of every word to the reducer. This approach, while easier to write and debug, is unlikely to be the best choice for a larger scale implementation because of the large volume of data that would have to be sent to the reducers. A potentially more-streamlined alternative would be to add a \"combiner\" step at the end of the mapper that would send one line for each word-email combination (E.G. Key:email-word-flag, Value:count).\n",
    "\n",
    "In addition, if we only care about generating the conditional probabilities for each word and don't need to classify all the training emails, we could simplify the implementation even more and not send the email contents themselves to the reducer. Not only would this decrease throughput, but it would also dramatically reduce the amount of information that the reducer would need to store in memory. In this situation, our mapper could simply emit words along with their conditional class counts. I have not implemented this in this homework, but we'll want to keep this in mind for the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: James Gray\n",
    "## Description: mapper code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #Compile regex to easily parse complete words\n",
    "filename = sys.argv[1]\n",
    "findwords = sys.argv[2].lower() \n",
    "with open (filename, \"r\") as myfile:\n",
    "    for num,line in enumerate(myfile.readlines()):\n",
    "        fields=line.split('\\t') #parse line into separate fields\n",
    "        subject_and_body=\" \".join(fields[-2:]).strip()#parse the subject & body fields, and combine into one string\n",
    "        words=re.findall(WORD_RE,subject_and_body) #create list of words\n",
    "        for word in words:\n",
    "            #This flag indicates to the reducer that a given word should be considered\n",
    "            #by the reducer when calculating the conditional probabilities\n",
    "            flag=0\n",
    "            if word in findwords:\n",
    "                flag=1 \n",
    "                \n",
    "            #This will send one row for every word instance to the reducer.\n",
    "            # ID \\t SPAM_flag \\t word \\t flag_for_reducer \n",
    "            print fields[0]+'\\t'+fields[1]+'\\t'+word+'\\t1\\t'+str(flag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 SPAM Classifier Reducer\n",
    "\n",
    "The reducer maintains two associative arrays: \n",
    "\n",
    "* The first stores information about each word, including how many times it appears in spam and ham messages, as well as if it's been flagged in the mapper. \n",
    "\n",
    "* The  second stores information about emails, including whether it is marked as spam, as well as a list of words it contains. \n",
    "\n",
    "As described above, a more scalable solution that does not need to maintain all the contents of the emails in memory for classification could simply calculate conditional probabilities \"lazily\" and only store the running probability values rather than the words themselves. Once all the data has arrived from the mappers, the array containing words is updated with the calculated conditional probabilities of spam and ham. At this point, the model is \"trained\". \n",
    "\n",
    "Finally, these conditional probabilities are reapplied to the word lists associated with each email to make the final spam/ham classification.\n",
    "\n",
    "Note: I have also included (in the comments) an alternative calculation for conditional probabilities that applies a Laplace smoothing approach, since I'd already implemented it before the assignment instructions were updated. I've done the same thing with the log probability calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW 1.3 - Reducer function\n",
    "\n",
    "from __future__ import division #Python 3-style division syntax is much cleaner\n",
    "import sys\n",
    "from math import log\n",
    " \n",
    "words={} # holds all words across the corpus\n",
    "emails={}\n",
    "spam_email_count=0 #number of emails marked as spam\n",
    "spam_word_count=0 #number of total (not unique) words in spam emails\n",
    "ham_word_count=0 #number of total (not unique) words in ham emails\n",
    "flagged_words=[]\n",
    "for chunk in sys.argv[1:]: #iterate through all of the output files generated by the mapper\n",
    "    with open (chunk, \"r\") as myfile:\n",
    "        for i in myfile.readlines():\n",
    "            \n",
    "            #parse the incoming line\n",
    "            result=i.split(\"\\t\")\n",
    "            email=result[0]\n",
    "            spam=int(result[1])  # spam/ham flag\n",
    "            word=result[2]       # word of interest\n",
    "            flag=int(result[4])  # flag from mapper denoting this is a word to process\n",
    "            \n",
    "            #initialize storage for word/email data\n",
    "            if word not in words.keys():\n",
    "                words[word]={'ham_count':0,'spam_count':0,'flag':flag}\n",
    "            if email not in emails.keys():\n",
    "                emails[email]={'spam':spam,'word_count':0,'words':[]}\n",
    "                if spam==1:\n",
    "                    spam_email_count+=1\n",
    "                \n",
    "            #store word data \n",
    "            if spam==1:\n",
    "                words[word]['spam_count']+=1\n",
    "                spam_word_count+=1\n",
    "            else:\n",
    "                words[word]['ham_count']+=1\n",
    "                ham_word_count+=1\n",
    "            \n",
    "            if flag==1 and word not in flagged_words:\n",
    "                flagged_words.append(word)\n",
    "                \n",
    "            #store email data \n",
    "            emails[email]['words'].append(word)\n",
    "            emails[email]['word_count']+=1\n",
    " \n",
    "#Calculate stats for entire corpus\n",
    "prior_spam=spam_email_count/len(emails) # prior probability of SPAM\n",
    "prior_ham=1-prior_spam # prior probability of HAM\n",
    "vocab_count=len(words)#number of unique words in the total vocabulary\n",
    "            \n",
    "for k,word in words.iteritems():\n",
    "    #These versions calculate conditional probabilities WITH Laplace smoothing.  \n",
    "    #word['p_spam']=(word['spam_count']+1)/(spam_word_count+vocab_count)\n",
    "    #word['p_ham']=(word['ham_count']+1)/(ham_word_count+vocab_count)\n",
    "    \n",
    "    #Compute conditional probabilities WITHOUT Laplace smoothing\n",
    "    word['p_spam']=(word['spam_count'])/(spam_word_count)\n",
    "    word['p_ham']=(word['ham_count'])/(ham_word_count)\n",
    " \n",
    "#At this point the model is now trained, and we can use it to make our predictions\n",
    "for j,email in emails.iteritems():\n",
    "    \n",
    "    #Log versions - previously used, but removed for now\n",
    "    #p_spam=log(prior_spam)\n",
    "    #p_ham=log(prior_ham)\n",
    "    \n",
    "    p_spam=prior_spam\n",
    "    p_ham=prior_ham\n",
    "    \n",
    "    for word in email['words']:\n",
    "        if word in flagged_words:\n",
    "            try:\n",
    "                #p_spam+=log(words[word]['p_spam']) #Log version - No longer used\n",
    "                p_spam*=(words[word]['p_spam'])\n",
    "            except ValueError:\n",
    "                continue #This means that words that do not appear in a class will use the class prior\n",
    "            try:\n",
    "                #p_ham+=log(words[word]['p_ham']) #Log version - No longer used\n",
    "                p_ham*=(words[word]['p_ham'])\n",
    "            except ValueError:\n",
    "                continue          \n",
    "    if p_spam>p_ham:\n",
    "        spam_pred=1\n",
    "    else:\n",
    "        spam_pred=0\n",
    "        \n",
    "    print j+'\\t'+str(email['spam'])+'\\t'+str(spam_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set execute priviles on mapper.py and reducer.py\n",
    "\n",
    "!chmod a+x mapper.py reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Execute Naive Bayes Classifier\n",
    "\n",
    "The output file produced by the reducer has the following structure:\n",
    "\n",
    "ID \\t SPAM \\t SPAM_PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 1.3 - Results\n",
      "0010.2003-12-18.GP\t1\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\n",
      "0001.2000-01-17.beck\t0\t1\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0005.1999-12-12.kaminski\t0\t1\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\n",
      "0008.2004-08-01.BG\t1\t1\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0017.2003-12-18.GP\t1\t0\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\n",
      "0015.2001-02-12.kitchen\t0\t1\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\n",
      "0007.2001-02-09.kitchen\t0\t1\n",
      "0016.2004-08-01.BG\t1\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0005.1999-12-14.farmer\t0\t1\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0013.2004-08-01.BG\t1\t1\n",
      "0005.2003-12-18.GP\t1\t1\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0003.2001-02-08.kitchen\t0\t1\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t1\n",
      "0014.2003-12-19.GP\t1\t0\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0010.2004-08-01.BG\t1\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0011.1999-12-14.farmer\t0\t1\n",
      "0013.1999-12-14.kaminski\t0\t1\n",
      "0001.2001-02-07.kitchen\t0\t1\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0007.2003-12-18.GP\t1\t0\n",
      "0017.2004-08-02.BG\t1\t1\n",
      "0014.2004-08-01.BG\t1\t0\n",
      "0006.2003-12-18.GP\t1\t0\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\n",
      "0008.2003-12-18.GP\t1\t0\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t1\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\n",
      "0015.1999-12-15.farmer\t0\t1\n",
      "0009.1999-12-13.kaminski\t0\t1\n",
      "0001.2000-06-06.lokay\t0\t1\n",
      "0011.2004-08-01.BG\t1\t0\n",
      "0004.2004-08-01.BG\t1\t1\n",
      "0018.2003-12-18.GP\t1\t1\n",
      "0002.1999-12-13.farmer\t0\t1\n",
      "0016.2003-12-19.GP\t1\t1\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0015.2003-12-19.GP\t1\t1\n",
      "0006.2004-08-01.BG\t1\t1\n",
      "0009.2003-12-18.GP\t1\t1\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t1\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0003.2004-08-01.BG\t1\t1\n",
      "0017.2004-08-01.BG\t1\t0\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0004.1999-12-10.kaminski\t0\t1\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0007.2004-08-01.BG\t1\t0\n",
      "0012.1999-12-14.kaminski\t0\t1\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0002.2003-12-18.GP\t1\t1\n",
      "0003.2003-12-18.GP\t1\t0\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0012.2003-12-19.GP\t1\t0\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\n",
      "0011.2003-12-18.GP\t1\t0\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\"\n",
    "!echo \"HW 1.3 - Results\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Training Error Calculation Function\n",
    "\n",
    "This function will be used to calculate the training error for the Naive Bayes models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "def calculate_training_error(pred, true):\n",
    "    \"\"\"Calculates the training error given a vector of predictions and a vector of true classes\"\"\"\n",
    "    \n",
    "    num_wrong=0\n",
    "    for i in zip(pred,true):\n",
    "        if i[0]!=i[1]: #If predicted value (i[0]) doesn't equal true value (i[1]), increment our count\n",
    "            num_wrong+=1\n",
    "            \n",
    "    #Divide number of incorrect examples by total number of examples in the data\n",
    "    print (\"Training error: \"+str(num_wrong/len(pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Calculate Training Error for Multinomial Naive Bayes model\n",
    "\n",
    "Interpretation of training error result: 38% of the predictions for SPAM are incorrect using the word \"assistance\" as an indicator for SPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB Results via Poor-Man's MapReduce Implementation using 'Assistance' only\n",
      "Training error: 0.38\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def eval_1_3():\n",
    "    with open('enronemail_1h.txt.output','rb') as f:\n",
    "        mr_data=pd.read_csv(f, sep='\\t', header=None)\n",
    "    print (\"Multinomial NB Results via Poor-Man's MapReduce Implementation using 'Assistance' only\")\n",
    "    calculate_training_error(mr_data[1],mr_data[2])\n",
    "\n",
    "eval_1_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.4. \n",
    "\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words.\n",
    "\n",
    "Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results (accuracy). To do so, make sure that:\n",
    "\n",
    "* mapper.py -> counts all occurrences of a list of words\n",
    "* reducer.py -> performs the multiple-word multinomial Naive Bayes classification via the chosen list. No smoothing is needed in this HW.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.4 SPAM Classifier Mapper\n",
    "\n",
    "This mapper function works very similarly to the implementation in 1.3. The only difference is that it enables iteration through a list of words (provided as arguments) for flagging for inclusion in the conditional probability calculation. In this example, the list of words is “assistance”, “valium”, and “enlargementWithATypo”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 1.4 - Mapper Function\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findwords = sys.argv[2].lower().split() \n",
    "with open (filename, \"r\") as myfile:\n",
    "    for num,line in enumerate(myfile.readlines()):\n",
    "        fields=line.split('\\t') #parse line into separate fields\n",
    "        #parse the subject and body fields from the line, and combine into one string\n",
    "        subject_and_body=\" \".join(fields[-2:]).strip()\n",
    "        words=re.findall(WORD_RE,subject_and_body)\n",
    "        for word in words:\n",
    "            flag=0\n",
    "            if word in findwords:\n",
    "                flag=1\n",
    "            print fields[0]+'\\t'+fields[1]+'\\t'+word+'\\t1\\t'+str(flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 SPAM Classifier Reducer\n",
    "\n",
    "This reducer is almost exactly the same as in Problem 1.3. The only difference is not in the code itself, but in the fact that it receives more than one flagged word from the mapper. Because the flagged words are tracked via a list, the reducer doesn't care how many flagged words it receives. It will incorporate all of them into the conditional probability calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 1.4 - Reducer Function\n",
    "from __future__ import division\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "#\n",
    "emails={} #Associative array to hold email data\n",
    "words={} #Associative array for word data\n",
    "\n",
    "spam_email_count=0 #number of emails marked as spam\n",
    "spam_word_count=0 #number of total (not unique) words in spam emails\n",
    "ham_word_count=0 #number of total (not unique) words in ham emails\n",
    "flagged_words=[] #list of flagged words to include in conditional probability calculation\n",
    "for chunk in sys.argv[1:]:\n",
    "    with open (chunk, \"r\") as myfile:\n",
    "        for i in myfile.readlines():\n",
    "            \n",
    "            #parse the line\n",
    "            result=i.split(\"\\t\")\n",
    "            email=result[0]\n",
    "            spam=int(result[1])\n",
    "            word=result[2]\n",
    "            flag=int(result[4])\n",
    "            \n",
    "            #initialize storage for word/email data\n",
    "            if word not in words.keys():\n",
    "                words[word]={'ham_count':0,'spam_count':0,'flag':flag}\n",
    "            if email not in emails.keys():\n",
    "                emails[email]={'spam':spam,'word_count':0,'words':[]}\n",
    "                if spam==1:\n",
    "                    spam_email_count+=1\n",
    "                \n",
    "            #store word data \n",
    "            if spam==1:\n",
    "                words[word]['spam_count']+=1\n",
    "                spam_word_count+=1\n",
    "            else:\n",
    "                words[word]['ham_count']+=1\n",
    "                ham_word_count+=1\n",
    "            \n",
    "            if flag==1 and word not in flagged_words:\n",
    "                flagged_words.append(word)\n",
    "                \n",
    "            #store email data \n",
    "            emails[email]['words'].append(word)\n",
    "            emails[email]['word_count']+=1\n",
    "\n",
    "#Calculate stats for entire corpus\n",
    "prior_spam=spam_email_count/len(emails)\n",
    "prior_ham=1-prior_spam\n",
    "vocab_count=len(words)#number of unique words in the total vocabulary\n",
    "            \n",
    "for k,word in words.iteritems():\n",
    "    #These versions calculate conditional probabilities WITH Laplace smoothing.  \n",
    "    #word['p_spam']=(word['spam_count']+1)/(spam_word_count+vocab_count)\n",
    "    #word['p_ham']=(word['ham_count']+1)/(ham_word_count+vocab_count)\n",
    "    \n",
    "    #Compute conditional probabilities WITHOUT Laplace smoothing\n",
    "    word['p_spam']=(word['spam_count'])/(spam_word_count)\n",
    "    word['p_ham']=(word['ham_count'])/(ham_word_count)\n",
    "\n",
    "#At this point the model is now trained, and we can use it to make our predictions\n",
    "for j,email in emails.iteritems():\n",
    "    \n",
    "    #Log versions - no longer used\n",
    "    #p_spam=log(prior_spam)\n",
    "    #p_ham=log(prior_ham)\n",
    "    \n",
    "    p_spam=prior_spam\n",
    "    p_ham=prior_ham\n",
    "    \n",
    "    for word in email['words']:\n",
    "        if word in flagged_words:\n",
    "            try:\n",
    "                #p_spam+=log(words[word]['p_spam']) #Log version - no longer used\n",
    "                p_spam*=words[word]['p_spam']\n",
    "            except ValueError:\n",
    "                pass #This means that words that do not appear in a class will use the class prior\n",
    "            try:\n",
    "                #p_ham+=log(words[word]['p_ham']) #Log version - no longer used\n",
    "                p_ham*=words[word]['p_ham']\n",
    "            except ValueError:\n",
    "                pass\n",
    "    if p_spam>p_ham:\n",
    "        spam_pred=1\n",
    "    else:\n",
    "        spam_pred=0\n",
    "        \n",
    "    print j+'\\t'+str(email['spam'])+'\\t'+str(spam_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set execute priviles on mapper.py and reducer.py\n",
    "\n",
    "!chmod a+x mapper.py reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Execute Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 1.4 - Results\n",
      "0010.2003-12-18.GP\t1\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0005.1999-12-12.kaminski\t0\t1\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\n",
      "0008.2004-08-01.BG\t1\t0\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0017.2003-12-18.GP\t1\t0\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0016.2004-08-01.BG\t1\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0013.2004-08-01.BG\t1\t1\n",
      "0005.2003-12-18.GP\t1\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "0014.2003-12-19.GP\t1\t0\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0010.2004-08-01.BG\t1\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0007.2003-12-18.GP\t1\t0\n",
      "0017.2004-08-02.BG\t1\t0\n",
      "0014.2004-08-01.BG\t1\t0\n",
      "0006.2003-12-18.GP\t1\t0\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\n",
      "0008.2003-12-18.GP\t1\t0\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0011.2004-08-01.BG\t1\t0\n",
      "0004.2004-08-01.BG\t1\t0\n",
      "0018.2003-12-18.GP\t1\t1\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0016.2003-12-19.GP\t1\t1\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0015.2003-12-19.GP\t1\t0\n",
      "0006.2004-08-01.BG\t1\t0\n",
      "0009.2003-12-18.GP\t1\t1\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0003.2004-08-01.BG\t1\t0\n",
      "0017.2004-08-01.BG\t1\t1\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0004.1999-12-10.kaminski\t0\t1\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0007.2004-08-01.BG\t1\t0\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0002.2003-12-18.GP\t1\t0\n",
      "0003.2003-12-18.GP\t1\t0\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0012.2003-12-19.GP\t1\t0\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\n",
      "0011.2003-12-18.GP\t1\t0\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance valium enlargementWithATypo\"\n",
    "!echo \"HW 1.4 - Results\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Calculate Training Error for Multinomial Naive Bayes model\n",
    "\n",
    "The training error was reduced by 1% (38% to 37%) by adding a few words to identify SPAM emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB Results via Poor-Man's MapReduce Implementation using 'Assistance valium EnlargementWithATypo'\n",
      "Training error: 0.37\n"
     ]
    }
   ],
   "source": [
    "#HW 1.4 - Evaluation code\n",
    "def eval_1_4():\n",
    "    with open('enronemail_1h.txt.output','rb') as f:\n",
    "        mr_data=pd.read_csv(f, sep='\\t', header=None)\n",
    "    print (\"Multinomial NB Results via Poor-Man's MapReduce Implementation using 'Assistance valium EnlargementWithATypo'\") \n",
    "    calculate_training_error(mr_data[1],mr_data[2])\n",
    "\n",
    "eval_1_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.5. \n",
    "\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present.\n",
    "\n",
    "To do so, make sure that:\n",
    "\n",
    "* mapper.py counts all occurrences of all words, and\n",
    "* reducer.py performs a word-distribution-wide Naive Bayes classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.5 SPAM Classifier Mapper\n",
    "\n",
    "This mapper will need to consider all words in the email instead of specific pre-defined words. The change from the other mappers is that the check for the word and flag for the reducer was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 1.5 - Mapper Function\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for num,line in enumerate(myfile.readlines()):\n",
    "        fields=line.split('\\t') #parse line into separate fields\n",
    "        # parse the subject and body fields from the line, and combine into one string\n",
    "        subject_and_body=\" \".join(fields[-2:]).strip()\n",
    "        words=re.findall(WORD_RE,subject_and_body)\n",
    "        for word in words:\n",
    "            # ID \\t SPAM_flag \\t word \n",
    "            print fields[0]+'\\t'+fields[1]+'\\t'+word+'\\t1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 SPAM Classifier Reducer\n",
    "\n",
    "The reducer is similar to the above reducers although the check for flagged words was removed as we need to calculate the conditional probabilities of all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 1.5 - Reducer Function\n",
    "from __future__ import division\n",
    "import sys\n",
    "from math import log\n",
    "emails={}\n",
    "words={}\n",
    "spam_email_count=0 #number of emails marked as spam\n",
    "spam_word_count=0 #number of total (not unique) words in spam emails\n",
    "ham_word_count=0 #number of total (not unique) words in ham emails\n",
    "\n",
    "for chunk in sys.argv[1:]:\n",
    "    with open (chunk, \"r\") as myfile:\n",
    "        for i in myfile.readlines():\n",
    "            \n",
    "            #parse the line\n",
    "            result=i.split(\"\\t\")\n",
    "            email=result[0]\n",
    "            spam=int(result[1])\n",
    "            word=result[2]\n",
    "            \n",
    "            #initialize storage for word/email data\n",
    "            if word not in words.keys():\n",
    "                words[word]={'ham_count':0,'spam_count':0}\n",
    "            if email not in emails.keys():\n",
    "                emails[email]={'spam':spam,'word_count':0,'words':[]}\n",
    "                if spam==1:\n",
    "                    spam_email_count+=1\n",
    "                \n",
    "            #store word data \n",
    "            if spam==1:\n",
    "                words[word]['spam_count']+=1\n",
    "                spam_word_count+=1\n",
    "            else:\n",
    "                words[word]['ham_count']+=1\n",
    "                ham_word_count+=1\n",
    "                \n",
    "            #store email data \n",
    "            emails[email]['words'].append(word)\n",
    "            emails[email]['word_count']+=1\n",
    "\n",
    "#Calculate stats for entire corpus\n",
    "prior_spam=spam_email_count/len(emails)\n",
    "prior_ham=1-prior_spam\n",
    "vocab_count=len(words)#number of unique words in the total vocabulary\n",
    "            \n",
    "for k,word in words.iteritems():\n",
    "    #These versions calculate conditional probabilities WITH Laplace smoothing.  \n",
    "    #word['p_spam']=(word['spam_count']+1)/(spam_word_count+vocab_count)\n",
    "    #word['p_ham']=(word['ham_count']+1)/(ham_word_count+vocab_count)\n",
    "    \n",
    "    #Compute conditional probabilities WITHOUT Laplace smoothing\n",
    "    word['p_spam']=(word['spam_count'])/(spam_word_count)\n",
    "    word['p_ham']=(word['ham_count'])/(ham_word_count)\n",
    "\n",
    "#At this point the model is now trained, and we can use it to make our dpredictions\n",
    "for j,email in emails.iteritems():\n",
    "    \n",
    "    #Log Version - not used\n",
    "    p_spam=log(prior_spam)\n",
    "    p_ham=log(prior_ham)\n",
    "    \n",
    "    p_spam=prior_spam\n",
    "    p_ham=prior_ham\n",
    "    for word in email['words']:\n",
    "        try:\n",
    "            #p_spam+=log(words[word]['p_spam']) #Log Version - not used\n",
    "            p_spam*=(words[word]['p_spam'])\n",
    "        except ValueError:\n",
    "            continue #This means that words that do not appear in a class will use the class prior\n",
    "        try:\n",
    "            #p_ham+=log(words[word]['p_ham']) #Log Version - not used\n",
    "            p_ham*=(words[word]['p_ham'])\n",
    "        except ValueError:\n",
    "            continue         \n",
    "    if p_spam>p_ham:\n",
    "        spam_pred=1\n",
    "    else:\n",
    "        spam_pred=0\n",
    "        \n",
    "    #print spam_pred, email['spam'],p_spam,p_ham,j\n",
    "    print j+'\\t'+str(email['spam'])+'\\t'+str(spam_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set execute priviles on mapper.py and reducer.py\n",
    "\n",
    "!chmod a+x mapper.py reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Execute Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0010.2003-12-18.GP\t1\t1\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t0\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t1\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0016.2004-08-01.BG\t1\t1\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0013.2004-08-01.BG\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0014.2003-12-19.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0011.2004-08-01.BG\t1\t1\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0018.2003-12-18.GP\t1\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t1\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0002.2004-08-01.BG\t1\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\r\n",
      "0011.2003-12-18.GP\t1\t1\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"*\"; \n",
    "! cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Calculate Training Error for Multinomial Naive Bayes model\n",
    "\n",
    "By considering all of the words the training error was reduced an additional 3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB Results via Poor-Man's MapReduce Implementation\n",
      "Training error: 0.34\n"
     ]
    }
   ],
   "source": [
    "def eval_1_5():\n",
    "    with open('enronemail_1h.txt.output','rb') as f:\n",
    "        mr_data=pd.read_csv(f, sep='\\t', header=None)\n",
    "    print (\"Multinomial NB Results via Poor-Man's MapReduce Implementation\")\n",
    "    calculate_training_error(mr_data[1],mr_data[2])\n",
    "\n",
    "eval_1_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.6 \n",
    "\n",
    "Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes.\n",
    "\n",
    "It always a good idea to test your solutions against publicly available libraries such as SciKit-Learn, The Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html more  \n",
    "\n",
    "Training error = misclassification rate with respect to a training set. It is more formally defined here:\n",
    "\n",
    "Let DF represent the training set in the following:\n",
    "Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”\n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "1. Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "2. Please prepare a table to present your results\n",
    "3. Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn (Hint: smoothing, which we will discuss in next lecture)\n",
    "4. Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.6 Data read and cleaning for SK Learn Multinomial NB Algorithm\n",
    "\n",
    "There are some lines with \"NA\" as the body and these should be removed to properly train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>spamflag</th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>subject_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001.1999-12-10.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>re: rankings</td>\n",
       "      <td>thank you.</td>\n",
       "      <td>re: rankings thank you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>leadership development pilot</td>\n",
       "      <td>sally:  what timing, ask and you shall receiv...</td>\n",
       "      <td>leadership development pilot sally:  what tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001.2001-02-07.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>key hr issues going forward</td>\n",
       "      <td>a) year end reviews-report needs generating l...</td>\n",
       "      <td>key hr issues going forward a) year end revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001.2001-04-02.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>re: quasi</td>\n",
       "      <td>good morning,  i'd love to go get some coffee...</td>\n",
       "      <td>re: quasi good morning,  i'd love to go get s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002.1999-12-13.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>vastar resources, inc.</td>\n",
       "      <td>gary, production from the high island larger ...</td>\n",
       "      <td>vastar resources, inc. gary, production from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0002.2001-02-07.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>congrats!</td>\n",
       "      <td>contratulations on the execution of the centr...</td>\n",
       "      <td>congrats! contratulations on the execution of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0002.2001-05-25.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>fw: this is the solution i mentioned lsc</td>\n",
       "      <td>oo  thank you,  your email address was obtain...</td>\n",
       "      <td>fw: this is the solution i mentioned lsc oo  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0002.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>adv: space saving computer to replace that bi...</td>\n",
       "      <td>revolutionary!!! full featured!!!  space savi...</td>\n",
       "      <td>adv: space saving computer to replace that bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0002.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>advs</td>\n",
       "      <td>greetings,  i am benedicta lindiwe hendricks ...</td>\n",
       "      <td>advs greetings,  i am benedicta lindiwe hendr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0003.1999-12-10.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>re: visit to enron</td>\n",
       "      <td>vince,  dec. 29 at 9:00 will be fine. i have ...</td>\n",
       "      <td>re: visit to enron vince,  dec. 29 at 9:00 wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0003.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>calpine daily gas nomination</td>\n",
       "      <td>-calpine daily gas nomination 1. doc</td>\n",
       "      <td>calpine daily gas nomination -calpine daily g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0003.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>re: additional responsibility</td>\n",
       "      <td>congratulations on this additional responsibi...</td>\n",
       "      <td>re: additional responsibility congratulations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0003.2001-02-08.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>re: key hr issues going forward</td>\n",
       "      <td>all is under control:  a-we've set up a \" wor...</td>\n",
       "      <td>re: key hr issues going forward all is under ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0003.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>fw: account over due wfxu ppmfztdtet</td>\n",
       "      <td>eliminate your credit card debt without bankr...</td>\n",
       "      <td>fw: account over due wfxu ppmfztdtet eliminat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0003.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>whats new in summer? bawled</td>\n",
       "      <td>carolyn regretful watchfully procrustes godly...</td>\n",
       "      <td>whats new in summer? bawled carolyn regretful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0004.1999-12-10.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>research group move to the 19 th floor</td>\n",
       "      <td>hello all:  in case any of you feel energetic...</td>\n",
       "      <td>research group move to the 19 th floor hello ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0004.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>re: issue</td>\n",
       "      <td>fyi-see note below-already done.  stella  ---...</td>\n",
       "      <td>re: issue fyi-see note below-already done.  s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0004.2001-04-02.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>enrononline desk to desk id and password</td>\n",
       "      <td>bill,  the epmi-st-wbom book has been set up ...</td>\n",
       "      <td>enrononline desk to desk id and password bill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0004.2001-06-12.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>spend too much on your phone bill? 25711</td>\n",
       "      <td>crystal clear connection with unlimited  long...</td>\n",
       "      <td>spend too much on your phone bill? 25711 crys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0005.1999-12-12.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>christmas baskets</td>\n",
       "      <td>the christmas baskets have been ordered.  we ...</td>\n",
       "      <td>christmas baskets the christmas baskets have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0005.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>meter 7268 nov allocation</td>\n",
       "      <td>fyi.  ----------------------forwarded by laur...</td>\n",
       "      <td>meter 7268 nov allocation fyi.  -------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0005.2000-06-06.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>transportation to resort</td>\n",
       "      <td>please be informed, a mini-bus has been reser...</td>\n",
       "      <td>transportation to resort please be informed, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0005.2001-02-08.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>epmi files protest of entergy transco</td>\n",
       "      <td>attached is our filing made yesterday protest...</td>\n",
       "      <td>epmi files protest of entergy transco attache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0005.2001-06-23.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>discounted mortgage broker 512517</td>\n",
       "      <td>moates are at an all tyone with any crest and...</td>\n",
       "      <td>discounted mortgage broker 512517 moates are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0005.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>miningnews. net newsletter-thursday, december...</td>\n",
       "      <td>thursday, december 18,2003 miningnews. net  t...</td>\n",
       "      <td>miningnews. net newsletter-thursday, december...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0006.1999-12-13.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>japan candidate</td>\n",
       "      <td>vince,  i spoke with whalley at the sa offsit...</td>\n",
       "      <td>japan candidate vince,  i spoke with whalley ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0006.2001-02-08.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>california power 2/8</td>\n",
       "      <td>please contact kristin walsh (x 39510) or rob...</td>\n",
       "      <td>california power 2/8 please contact kristin w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0006.2001-04-03.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>david gray</td>\n",
       "      <td>bill,  is this the david gray you are going t...</td>\n",
       "      <td>david gray bill,  is this the david gray you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0006.2001-06-25.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>looking 4 real fun 211075433222</td>\n",
       "      <td>talk on tele with locals in your area who wan...</td>\n",
       "      <td>looking 4 real fun 211075433222 talk on tele ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0006.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>dobmeos with hgh my energy level has gone up!...</td>\n",
       "      <td>introducing  doctor-formulated  hgh  human gr...</td>\n",
       "      <td>dobmeos with hgh my energy level has gone up!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0013.2001-04-03.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>re: monday blues</td>\n",
       "      <td>good morning. i'm glad to hear that you are h...</td>\n",
       "      <td>re: monday blues good morning. i'm glad to he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0013.2001-06-30.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>your membership community charset = iso-8859-1</td>\n",
       "      <td>your membership community &amp; commentary (june ...</td>\n",
       "      <td>your membership community charset = iso-8859-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0013.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>take the reins</td>\n",
       "      <td>become  your employer.  substantial profit pr...</td>\n",
       "      <td>take the reins become  your employer.  substa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0014.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>re: new color printer</td>\n",
       "      <td>sorry,  don't we need to know the cost, as we...</td>\n",
       "      <td>re: new color printer sorry,  don't we need t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0014.1999-12-15.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>lst rev dec. 1999 josey ranch nom</td>\n",
       "      <td>fyi  ----------------------forwarded by susan...</td>\n",
       "      <td>lst rev dec. 1999 josey ranch nom fyi  ------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0014.2001-02-12.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>correction--conference call on tuesday, febru...</td>\n",
       "      <td>1)  ssb conference call  tuesday, february 13...</td>\n",
       "      <td>correction--conference call on tuesday, febru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0014.2001-07-04.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>new accounts # 2 c 6 e</td>\n",
       "      <td>this is a mime message  content-type: multipa...</td>\n",
       "      <td>new accounts # 2 c 6 e this is a mime message...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0014.2003-12-19.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>get g: eneric via-gra for a s low as$ 2. 50 p...</td>\n",
       "      <td>her type  http:// dutchess. reado 893. com/xm /</td>\n",
       "      <td>get g: eneric via-gra for a s low as$ 2. 50 p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0014.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>today</td>\n",
       "      <td>hey,  last week, jo and me were talking about...</td>\n",
       "      <td>today hey,  last week, jo and me were talking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0015.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>imperial capital-thursday schedule</td>\n",
       "      <td>the following is the schedule for thursday's ...</td>\n",
       "      <td>imperial capital-thursday schedule the follow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0015.1999-12-15.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>2 nd rev dec. 1999 josey ranch nom</td>\n",
       "      <td>----------------------forwarded by susan d tr...</td>\n",
       "      <td>2 nd rev dec. 1999 josey ranch nom ----------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0015.2000-06-09.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>tw weekly, 6-9-00</td>\n",
       "      <td>please see the attached file and let me know ...</td>\n",
       "      <td>tw weekly, 6-9-00 please see the attached fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0015.2001-02-12.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>california update 2/12</td>\n",
       "      <td>executive summary:  the likelihood of there b...</td>\n",
       "      <td>california update 2/12 executive summary:  th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0015.2001-07-05.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>get the best rate on a home loan!</td>\n",
       "      <td>if you would like to be removed  from future ...</td>\n",
       "      <td>get the best rate on a home loan! if you woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0015.2003-12-19.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>mr. uwe schmidt is a knave! don't buy any pro...</td>\n",
       "      <td>dear friends,  microsale sc kg, ltd, germany ...</td>\n",
       "      <td>mr. uwe schmidt is a knave! don't buy any pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0016.1999-12-15.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>unify close schedule</td>\n",
       "      <td>the following is the close schedule for this ...</td>\n",
       "      <td>unify close schedule the following is the clo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0016.2001-02-12.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>fw: meeting with jeff skilling</td>\n",
       "      <td>louise,  per our conversation of last week, y...</td>\n",
       "      <td>fw: meeting with jeff skilling louise,  per o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0016.2001-07-05.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>get the best rate on a home loan!</td>\n",
       "      <td>if you would like to be removed  from future ...</td>\n",
       "      <td>get the best rate on a home loan! if you woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0016.2001-07-06.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>your membership community charset = iso-8859-1</td>\n",
       "      <td>your membership community &amp; commentary (july ...</td>\n",
       "      <td>your membership community charset = iso-8859-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0016.2003-12-19.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>ativan. n vicodin. n xanax. x valium. m dxqrgu</td>\n",
       "      <td>many specials running this week  the re. al t...</td>\n",
       "      <td>ativan. n vicodin. n xanax. x valium. m dxqrg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0016.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>important news for usavity customers.</td>\n",
       "      <td>dear cheapsoft customer,  my name is annie ki...</td>\n",
       "      <td>important news for usavity customers. dear ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0017.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>a paper of mine</td>\n",
       "      <td>vince,  i have written a paper, which suppose...</td>\n",
       "      <td>a paper of mine vince,  i have written a pape...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0017.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>global risk management operations</td>\n",
       "      <td>congratulations, sally!!!  kk  --------------...</td>\n",
       "      <td>global risk management operations congratulat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0017.2001-04-03.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>monday blues</td>\n",
       "      <td>bill,  i am having such a terrible day. i am ...</td>\n",
       "      <td>monday blues bill,  i am having such a terrib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0017.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>get that new car 8434</td>\n",
       "      <td>people nowthe weather or climate in any parti...</td>\n",
       "      <td>get that new car 8434 people nowthe weather o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0017.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>super-discounts on ambien and soma</td>\n",
       "      <td>up to 80%  savings on  xanax, valium, phenter...</td>\n",
       "      <td>super-discounts on ambien and soma up to 80% ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0017.2004-08-02.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>your winning notice.</td>\n",
       "      <td>pacific international lottery organisation.  ...</td>\n",
       "      <td>your winning notice. pacific international lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0018.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>invitation to speak at power 2000</td>\n",
       "      <td>hi vince  it is my great pleasure to invite y...</td>\n",
       "      <td>invitation to speak at power 2000 hi vince  i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0018.2001-07-13.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>[ilug] we need your assistance to invest in y...</td>\n",
       "      <td>dear sir/madam,  i am well confident of your ...</td>\n",
       "      <td>[ilug] we need your assistance to invest in y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0018.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>await your response</td>\n",
       "      <td>dear partner,  we are a team of government of...</td>\n",
       "      <td>await your response dear partner,  we are a t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  spamflag  \\\n",
       "1    0001.1999-12-10.kaminski         0   \n",
       "2        0001.2000-01-17.beck         0   \n",
       "4     0001.2001-02-07.kitchen         0   \n",
       "5    0001.2001-04-02.williams         0   \n",
       "6      0002.1999-12-13.farmer         0   \n",
       "7     0002.2001-02-07.kitchen         0   \n",
       "8   0002.2001-05-25.SA_and_HP         1   \n",
       "9          0002.2003-12-18.GP         1   \n",
       "10         0002.2004-08-01.BG         1   \n",
       "11   0003.1999-12-10.kaminski         0   \n",
       "12     0003.1999-12-14.farmer         0   \n",
       "13       0003.2000-01-17.beck         0   \n",
       "14    0003.2001-02-08.kitchen         0   \n",
       "15         0003.2003-12-18.GP         1   \n",
       "16         0003.2004-08-01.BG         1   \n",
       "17   0004.1999-12-10.kaminski         0   \n",
       "18     0004.1999-12-14.farmer         0   \n",
       "19   0004.2001-04-02.williams         0   \n",
       "20  0004.2001-06-12.SA_and_HP         1   \n",
       "22   0005.1999-12-12.kaminski         0   \n",
       "23     0005.1999-12-14.farmer         0   \n",
       "24      0005.2000-06-06.lokay         0   \n",
       "25    0005.2001-02-08.kitchen         0   \n",
       "26  0005.2001-06-23.SA_and_HP         1   \n",
       "27         0005.2003-12-18.GP         1   \n",
       "28   0006.1999-12-13.kaminski         0   \n",
       "29    0006.2001-02-08.kitchen         0   \n",
       "30   0006.2001-04-03.williams         0   \n",
       "31  0006.2001-06-25.SA_and_HP         1   \n",
       "32         0006.2003-12-18.GP         1   \n",
       "..                        ...       ...   \n",
       "70   0013.2001-04-03.williams         0   \n",
       "71  0013.2001-06-30.SA_and_HP         1   \n",
       "72         0013.2004-08-01.BG         1   \n",
       "73   0014.1999-12-14.kaminski         0   \n",
       "74     0014.1999-12-15.farmer         0   \n",
       "75    0014.2001-02-12.kitchen         0   \n",
       "76  0014.2001-07-04.SA_and_HP         1   \n",
       "77         0014.2003-12-19.GP         1   \n",
       "78         0014.2004-08-01.BG         1   \n",
       "79   0015.1999-12-14.kaminski         0   \n",
       "80     0015.1999-12-15.farmer         0   \n",
       "81      0015.2000-06-09.lokay         0   \n",
       "82    0015.2001-02-12.kitchen         0   \n",
       "83  0015.2001-07-05.SA_and_HP         1   \n",
       "84         0015.2003-12-19.GP         1   \n",
       "85     0016.1999-12-15.farmer         0   \n",
       "86    0016.2001-02-12.kitchen         0   \n",
       "87  0016.2001-07-05.SA_and_HP         1   \n",
       "88  0016.2001-07-06.SA_and_HP         1   \n",
       "89         0016.2003-12-19.GP         1   \n",
       "90         0016.2004-08-01.BG         1   \n",
       "91   0017.1999-12-14.kaminski         0   \n",
       "92       0017.2000-01-17.beck         0   \n",
       "93   0017.2001-04-03.williams         0   \n",
       "94         0017.2003-12-18.GP         1   \n",
       "95         0017.2004-08-01.BG         1   \n",
       "96         0017.2004-08-02.BG         1   \n",
       "97   0018.1999-12-14.kaminski         0   \n",
       "98  0018.2001-07-13.SA_and_HP         1   \n",
       "99         0018.2003-12-18.GP         1   \n",
       "\n",
       "                                              subject  \\\n",
       "1                                        re: rankings   \n",
       "2                        leadership development pilot   \n",
       "4                         key hr issues going forward   \n",
       "5                                           re: quasi   \n",
       "6                              vastar resources, inc.   \n",
       "7                                           congrats!   \n",
       "8            fw: this is the solution i mentioned lsc   \n",
       "9    adv: space saving computer to replace that bi...   \n",
       "10                                               advs   \n",
       "11                                 re: visit to enron   \n",
       "12                       calpine daily gas nomination   \n",
       "13                      re: additional responsibility   \n",
       "14                    re: key hr issues going forward   \n",
       "15               fw: account over due wfxu ppmfztdtet   \n",
       "16                        whats new in summer? bawled   \n",
       "17             research group move to the 19 th floor   \n",
       "18                                          re: issue   \n",
       "19           enrononline desk to desk id and password   \n",
       "20           spend too much on your phone bill? 25711   \n",
       "22                                  christmas baskets   \n",
       "23                          meter 7268 nov allocation   \n",
       "24                           transportation to resort   \n",
       "25              epmi files protest of entergy transco   \n",
       "26                  discounted mortgage broker 512517   \n",
       "27   miningnews. net newsletter-thursday, december...   \n",
       "28                                    japan candidate   \n",
       "29                               california power 2/8   \n",
       "30                                         david gray   \n",
       "31                    looking 4 real fun 211075433222   \n",
       "32   dobmeos with hgh my energy level has gone up!...   \n",
       "..                                                ...   \n",
       "70                                   re: monday blues   \n",
       "71     your membership community charset = iso-8859-1   \n",
       "72                                     take the reins   \n",
       "73                              re: new color printer   \n",
       "74                  lst rev dec. 1999 josey ranch nom   \n",
       "75   correction--conference call on tuesday, febru...   \n",
       "76                             new accounts # 2 c 6 e   \n",
       "77   get g: eneric via-gra for a s low as$ 2. 50 p...   \n",
       "78                                              today   \n",
       "79                 imperial capital-thursday schedule   \n",
       "80                 2 nd rev dec. 1999 josey ranch nom   \n",
       "81                                  tw weekly, 6-9-00   \n",
       "82                             california update 2/12   \n",
       "83                  get the best rate on a home loan!   \n",
       "84   mr. uwe schmidt is a knave! don't buy any pro...   \n",
       "85                               unify close schedule   \n",
       "86                     fw: meeting with jeff skilling   \n",
       "87                  get the best rate on a home loan!   \n",
       "88     your membership community charset = iso-8859-1   \n",
       "89     ativan. n vicodin. n xanax. x valium. m dxqrgu   \n",
       "90              important news for usavity customers.   \n",
       "91                                    a paper of mine   \n",
       "92                  global risk management operations   \n",
       "93                                       monday blues   \n",
       "94                              get that new car 8434   \n",
       "95                 super-discounts on ambien and soma   \n",
       "96                               your winning notice.   \n",
       "97                  invitation to speak at power 2000   \n",
       "98   [ilug] we need your assistance to invest in y...   \n",
       "99                                await your response   \n",
       "\n",
       "                                                 body  \\\n",
       "1                                          thank you.   \n",
       "2    sally:  what timing, ask and you shall receiv...   \n",
       "4    a) year end reviews-report needs generating l...   \n",
       "5    good morning,  i'd love to go get some coffee...   \n",
       "6    gary, production from the high island larger ...   \n",
       "7    contratulations on the execution of the centr...   \n",
       "8    oo  thank you,  your email address was obtain...   \n",
       "9    revolutionary!!! full featured!!!  space savi...   \n",
       "10   greetings,  i am benedicta lindiwe hendricks ...   \n",
       "11   vince,  dec. 29 at 9:00 will be fine. i have ...   \n",
       "12               -calpine daily gas nomination 1. doc   \n",
       "13   congratulations on this additional responsibi...   \n",
       "14   all is under control:  a-we've set up a \" wor...   \n",
       "15   eliminate your credit card debt without bankr...   \n",
       "16   carolyn regretful watchfully procrustes godly...   \n",
       "17   hello all:  in case any of you feel energetic...   \n",
       "18   fyi-see note below-already done.  stella  ---...   \n",
       "19   bill,  the epmi-st-wbom book has been set up ...   \n",
       "20   crystal clear connection with unlimited  long...   \n",
       "22   the christmas baskets have been ordered.  we ...   \n",
       "23   fyi.  ----------------------forwarded by laur...   \n",
       "24   please be informed, a mini-bus has been reser...   \n",
       "25   attached is our filing made yesterday protest...   \n",
       "26   moates are at an all tyone with any crest and...   \n",
       "27   thursday, december 18,2003 miningnews. net  t...   \n",
       "28   vince,  i spoke with whalley at the sa offsit...   \n",
       "29   please contact kristin walsh (x 39510) or rob...   \n",
       "30   bill,  is this the david gray you are going t...   \n",
       "31   talk on tele with locals in your area who wan...   \n",
       "32   introducing  doctor-formulated  hgh  human gr...   \n",
       "..                                                ...   \n",
       "70   good morning. i'm glad to hear that you are h...   \n",
       "71   your membership community & commentary (june ...   \n",
       "72   become  your employer.  substantial profit pr...   \n",
       "73   sorry,  don't we need to know the cost, as we...   \n",
       "74   fyi  ----------------------forwarded by susan...   \n",
       "75   1)  ssb conference call  tuesday, february 13...   \n",
       "76   this is a mime message  content-type: multipa...   \n",
       "77   her type  http:// dutchess. reado 893. com/xm /    \n",
       "78   hey,  last week, jo and me were talking about...   \n",
       "79   the following is the schedule for thursday's ...   \n",
       "80   ----------------------forwarded by susan d tr...   \n",
       "81   please see the attached file and let me know ...   \n",
       "82   executive summary:  the likelihood of there b...   \n",
       "83   if you would like to be removed  from future ...   \n",
       "84   dear friends,  microsale sc kg, ltd, germany ...   \n",
       "85   the following is the close schedule for this ...   \n",
       "86   louise,  per our conversation of last week, y...   \n",
       "87   if you would like to be removed  from future ...   \n",
       "88   your membership community & commentary (july ...   \n",
       "89   many specials running this week  the re. al t...   \n",
       "90   dear cheapsoft customer,  my name is annie ki...   \n",
       "91   vince,  i have written a paper, which suppose...   \n",
       "92   congratulations, sally!!!  kk  --------------...   \n",
       "93   bill,  i am having such a terrible day. i am ...   \n",
       "94   people nowthe weather or climate in any parti...   \n",
       "95   up to 80%  savings on  xanax, valium, phenter...   \n",
       "96   pacific international lottery organisation.  ...   \n",
       "97   hi vince  it is my great pleasure to invite y...   \n",
       "98   dear sir/madam,  i am well confident of your ...   \n",
       "99   dear partner,  we are a team of government of...   \n",
       "\n",
       "                                         subject_body  \n",
       "1                             re: rankings thank you.  \n",
       "2    leadership development pilot sally:  what tim...  \n",
       "4    key hr issues going forward a) year end revie...  \n",
       "5    re: quasi good morning,  i'd love to go get s...  \n",
       "6    vastar resources, inc. gary, production from ...  \n",
       "7    congrats! contratulations on the execution of...  \n",
       "8    fw: this is the solution i mentioned lsc oo  ...  \n",
       "9    adv: space saving computer to replace that bi...  \n",
       "10   advs greetings,  i am benedicta lindiwe hendr...  \n",
       "11   re: visit to enron vince,  dec. 29 at 9:00 wi...  \n",
       "12   calpine daily gas nomination -calpine daily g...  \n",
       "13   re: additional responsibility congratulations...  \n",
       "14   re: key hr issues going forward all is under ...  \n",
       "15   fw: account over due wfxu ppmfztdtet eliminat...  \n",
       "16   whats new in summer? bawled carolyn regretful...  \n",
       "17   research group move to the 19 th floor hello ...  \n",
       "18   re: issue fyi-see note below-already done.  s...  \n",
       "19   enrononline desk to desk id and password bill...  \n",
       "20   spend too much on your phone bill? 25711 crys...  \n",
       "22   christmas baskets the christmas baskets have ...  \n",
       "23   meter 7268 nov allocation fyi.  -------------...  \n",
       "24   transportation to resort please be informed, ...  \n",
       "25   epmi files protest of entergy transco attache...  \n",
       "26   discounted mortgage broker 512517 moates are ...  \n",
       "27   miningnews. net newsletter-thursday, december...  \n",
       "28   japan candidate vince,  i spoke with whalley ...  \n",
       "29   california power 2/8 please contact kristin w...  \n",
       "30   david gray bill,  is this the david gray you ...  \n",
       "31   looking 4 real fun 211075433222 talk on tele ...  \n",
       "32   dobmeos with hgh my energy level has gone up!...  \n",
       "..                                                ...  \n",
       "70   re: monday blues good morning. i'm glad to he...  \n",
       "71   your membership community charset = iso-8859-...  \n",
       "72   take the reins become  your employer.  substa...  \n",
       "73   re: new color printer sorry,  don't we need t...  \n",
       "74   lst rev dec. 1999 josey ranch nom fyi  ------...  \n",
       "75   correction--conference call on tuesday, febru...  \n",
       "76   new accounts # 2 c 6 e this is a mime message...  \n",
       "77   get g: eneric via-gra for a s low as$ 2. 50 p...  \n",
       "78   today hey,  last week, jo and me were talking...  \n",
       "79   imperial capital-thursday schedule the follow...  \n",
       "80   2 nd rev dec. 1999 josey ranch nom ----------...  \n",
       "81   tw weekly, 6-9-00 please see the attached fil...  \n",
       "82   california update 2/12 executive summary:  th...  \n",
       "83   get the best rate on a home loan! if you woul...  \n",
       "84   mr. uwe schmidt is a knave! don't buy any pro...  \n",
       "85   unify close schedule the following is the clo...  \n",
       "86   fw: meeting with jeff skilling louise,  per o...  \n",
       "87   get the best rate on a home loan! if you woul...  \n",
       "88   your membership community charset = iso-8859-...  \n",
       "89   ativan. n vicodin. n xanax. x valium. m dxqrg...  \n",
       "90   important news for usavity customers. dear ch...  \n",
       "91   a paper of mine vince,  i have written a pape...  \n",
       "92   global risk management operations congratulat...  \n",
       "93   monday blues bill,  i am having such a terrib...  \n",
       "94   get that new car 8434 people nowthe weather o...  \n",
       "95   super-discounts on ambien and soma up to 80% ...  \n",
       "96   your winning notice. pacific international lo...  \n",
       "97   invitation to speak at power 2000 hi vince  i...  \n",
       "98   [ilug] we need your assistance to invest in y...  \n",
       "99   await your response dear partner,  we are a t...  \n",
       "\n",
       "[94 rows x 5 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('enronemail_1h.txt','rb') as f:\n",
    "    data=pd.read_csv(f, sep='\\t', header=None, na_filter=True)\n",
    "\n",
    "    # add column names to DataFrame\n",
    "df_columns = [\"id\", \"spamflag\", \"subject\", \"body\"]\n",
    "data.columns = df_columns\n",
    "# concatenate subject and body to create a text string to be evaluated by classifier\n",
    "data['subject_body'] = data[\"subject\"] + data[\"body\"]\n",
    "dataClean = data.dropna()\n",
    "dataClean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Create text features using CountVectorizer\n",
    "\n",
    "The [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) converts a collection of text documents to a matrix of token counts. Here it will process the 'subject_body' field of the DataFrame to create a vector of words and their counts. The fit_transform method creates a document-term matrix on the number of times a specific word (feature) appears in the document.  In this scenario we have 100 or less records given that we filtered out the records where an \"NA\" appears in the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3886)\t1\n",
      "  (0, 3871)\t1\n",
      "  (0, 4731)\t1\n",
      "  (0, 5255)\t1\n",
      "  (1, 4731)\t1\n",
      "  (1, 5255)\t5\n",
      "  (1, 2838)\t7\n",
      "  (1, 1524)\t3\n",
      "  (1, 3606)\t6\n",
      "  (1, 4167)\t1\n",
      "  (1, 5136)\t1\n",
      "  (1, 4785)\t1\n",
      "  (1, 625)\t1\n",
      "  (1, 526)\t11\n",
      "  (1, 4301)\t1\n",
      "  (1, 3914)\t2\n",
      "  (1, 619)\t3\n",
      "  (1, 3552)\t1\n",
      "  (1, 3448)\t2\n",
      "  (1, 1581)\t1\n",
      "  (1, 2910)\t3\n",
      "  (1, 785)\t3\n",
      "  (1, 2651)\t4\n",
      "  (1, 518)\t2\n",
      "  (1, 4967)\t2\n",
      "  :\t:\n",
      "  (93, 2883)\t1\n",
      "  (93, 1233)\t1\n",
      "  (93, 4854)\t2\n",
      "  (93, 565)\t1\n",
      "  (93, 3824)\t1\n",
      "  (93, 3678)\t1\n",
      "  (93, 607)\t1\n",
      "  (93, 4633)\t1\n",
      "  (93, 3840)\t1\n",
      "  (93, 1652)\t1\n",
      "  (93, 4717)\t1\n",
      "  (93, 2081)\t1\n",
      "  (93, 2633)\t1\n",
      "  (93, 3553)\t1\n",
      "  (93, 1261)\t1\n",
      "  (93, 395)\t1\n",
      "  (93, 1190)\t1\n",
      "  (93, 4506)\t1\n",
      "  (93, 1399)\t1\n",
      "  (93, 382)\t1\n",
      "  (93, 729)\t1\n",
      "  (93, 1392)\t1\n",
      "  (93, 103)\t1\n",
      "  (93, 271)\t1\n",
      "  (93, 280)\t1\n"
     ]
    }
   ],
   "source": [
    "# create matrix for a single word, convert to lowercase first, filter words with document frequency < 3\n",
    "vectorizer = CountVectorizer(analyzer='word', lowercase=True)\n",
    "# create a document-term matrix \n",
    "vocabulary = vectorizer.fit_transform(dataClean['subject_body'])\n",
    "print (vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Run SK Learn Multinomial Naive Bayer Classifier\n",
    "\n",
    "The [multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) algorithm uses word counts for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK Learn Multinomial NB training error: 0.0\n"
     ]
    }
   ],
   "source": [
    "multiNB = MultinomialNB()\n",
    "# fit the model to the training data using document-term matrix and class of whether SPAM or not\n",
    "multiNB.fit(vocabulary, dataClean['spamflag'])\n",
    "# make predictions using the training data\n",
    "mnbclf_results = multiNB.predict(vocabulary)\n",
    "# calculate accuracy using metrics libary\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (\"SK Learn Multinomial NB training error: \" + str(1-accuracy_score(dataClean['spamflag'],mnbclf_results)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Run SK Learn Bernoulli Naive Bayer Classifier\n",
    "\n",
    "The [bernoulli Naives Bayes](http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html) algorithm generates either a 1 for the presence of the term in a document or 0 for an absence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK Learn Bernoulli NB training error: 0.234042553191\n"
     ]
    }
   ],
   "source": [
    "# instantiate a bernoulli model\n",
    "bernNB = BernoulliNB()\n",
    "# fit the model to the training data using\n",
    "bernNB.fit(vocabulary, dataClean['spamflag'])\n",
    "bernclf_results = bernNB.predict(vocabulary)\n",
    "# calculate accuracy using metrics libary\n",
    "print (\"SK Learn Bernoulli NB training error: \" + str(1-accuracy_score(dataClean['spamflag'],bernclf_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Summary of Error Results and Conclusions\n",
    "\n",
    "This section compares and contrasts errors produces by map-reduce and scikit-learn\n",
    "\n",
    "\n",
    "| Model                                                                      | Training Error |\n",
    "|----------------------------------------------------------------------------|----------------|\n",
    "| Multinomial NB, Scikit-Learn Implementation                                | 0.00            |\n",
    "| Bernoulli NB, Scikit-Learn Implementation                                  | 0.23           |\n",
    "| Multinomial NB HW1.5, MapReduce implementation                                   | 0.34           |\n",
    "\n",
    "#### Analysis of Multinomial Naive Bayes models (MapReduce and scikit-learn)\n",
    "The map-reduce Multinomial Naive Bayes model accuracy was significantly higher than the scikit-learn implementation. A few considerations come to mind why there was a difference. In the map-reduce implementation we did not design for Laplace smoothing and therefore terms that do not exist in the vocabulary produce a zero probability. Laplace smoothing is set by default in the scikit-learn implementation. Feature extraction may have also played a role. In the map-reduce implementation we used regex to parse for words while scikit-learn's countvectorizer includes digits in words while ignoring hyphenations and contractions.  This can result in larger and richer vocabulary resulting in lower error.\n",
    "\n",
    "#### Analysis of scikit-learn Bernoulli and Multinomial Naive Bayes models (scikit-learn)\n",
    "The error of the multinomial and bernoulli naive bayes models implemented in scikit-learn differ significantly. The multinomial model error was 0 (not realistic in the real world because in this case we tested the model with training data) while the bernoulli model was 0.23.  The primary reason is the multinomial model uses word counts to estimate probabilities while the bernoulli model is binary if the word appears in the document or not. This enables the multinomial model to discriminate at a deeper level and we should expect to see a lower error when compared to the Bernoulli model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
