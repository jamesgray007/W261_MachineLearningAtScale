{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS UC Berkeley - Machine Learning at Scale\n",
    "## DATSCIW261 ASSIGNMENT #5  \n",
    "\n",
    "[James Gray](https://github.com/jamesgray007)   \n",
    "jamesgray@ischool.berkeley.edu   \n",
    "Time of Initial Submission: 08:00 PM US Central, Sunday, June 20, 2016  \n",
    "Time of **Resubmission**:  \n",
    "W261-1, Spring 2016  \n",
    "Week 5 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.0\n",
    "\n",
    "**_What is a data warehouse?_** \n",
    "\n",
    "Bill Inmon's classic definition is that a data warehouse is a subject oriented, integrated, nonvolatile, time variant collection of data in support of management's decisions\" (Building the Data Warehouse,\" John Wiley & Sons, Inc., 1992).  A data warehouse integrates multiple data sources across the enterprise including OLTP and unstructured data such as weblogs. Data marts and cubes are built off the data warehouse to enable detailed analytics is specific subject domains such as sales reporting and analytics. A data warehouse is non-volatile and typically stores data over time periods. Data warehouses enable business intelligence but also machine learning workflows given the rich set of enterprise data.\n",
    "\n",
    "![DW](img/DW.jpg)\n",
    "\n",
    "\n",
    "**_What is a Star schema?_** \n",
    "\n",
    "The star schema is one of the more popular Zdata models found within the data warehousing. The star schema is represented by facts and dimensions as shown in the picture below.  The \"fact\" table contains the business measures of interest and the dimensions are foreign keys into tables that are attributes of the data and are used for slicing and dicing the measures. Facts are measures such as total sales, customer volume and units sold.  The dimensions enable aggregation and hierarchies such as viewing sales over different time periods (e.g. year, month, week, day). Other typical dimensions include geopgrahy, customer type, product and store. Star schemas are denormalized constructs purposely built for analytics, performance and straighforward queries instead of detailed join conditions.\n",
    "\n",
    "![](img/star.png)\n",
    "\n",
    "**_When is it used?_**\n",
    "\n",
    "Star schemas are used for business reporting and analytics where there are well-defined measures and dimensions. They are used when high performance aggregations and query performance is required such as viewing sales over various time periods across various dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# HW 5.1\n",
    "\n",
    "**_In the database world What is 3NF? Does machine learning use data in 3NF? If so why?_** \n",
    "\n",
    "Third normal form is a normal form that is used in normalizing a database design to reduce the duplication of data and ensure referential integrity by ensuring that (1) the entity is in second normal form, and (2) all the attributes in a table are determined only by the candidate keys of that table and not by any non-prime attributes ([Wikipedia](https://en.wikipedia.org/wiki/Third_normal_form)). The normalized data is split across multiple tables and queries are required to assemble the denormalized data for viewing or reporting purposes.  Machine learning does not use 3NF data given the normalized design and denormalized data sets are required to easily process the data.\n",
    "\n",
    "\n",
    "**_In what form does ML consume data?_**\n",
    "\n",
    "Machine learning models generally consume data as rows of tabular data where each line represents a complete record.  This is represnted as denormalized data if the data was joined from multiple tables of a 3NF design.\n",
    "\n",
    "**_Why would one use log files that are denormalized?_**\n",
    "\n",
    "Web logs files by themselves could enable insight but denomalized log files that include other data elements such as user attributes (e.g., name, location, age, ethnicity) would enable deeper analysis such as segmentation, sales campaigns, targeting selling, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.2\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.  Please report the number of rows resulting from:\n",
    "\n",
    "(1) Left joining Table Left with Table Right   \n",
    "(2) Right joining Table Left with Table Right   \n",
    "(3) Inner joining Table Left with Table Right\n",
    "\n",
    "======================================================================================================================\n",
    "\n",
    "### Join Data Analysis\n",
    "\n",
    "* Web Page Master (msft_urls.txt) = 294 records (Note from HW4.4 that there are 9 pages did not have visitor)    \n",
    "* Transformed Log file (ms_logs.txt) = 98,654 records\n",
    "\n",
    "Given that we are performing a memory-backed join, we will need to read the smaller file into memory.  Therefor we will read the most frequent visitor file into memory and stream over the log file to perform the log.\n",
    "\n",
    "![joins](img/joins.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.2 - Left Hashside Join (memory-backed map-side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting LeftJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile LeftJoin.py\n",
    "#!/usr/bin/python\n",
    "## leftjoin.py\n",
    "## Author: James Gray\n",
    "## Description: perform left hashside join on web page master and web page visits\n",
    "\n",
    "import csv\n",
    "#import LeftJoin\n",
    "#reload(LeftJoin)\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class LeftJoin(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        \"\"\"Load file of page of master URLs into memory\"\"\"\n",
    "        with open('msft_urls.txt','rb') as f:\n",
    "            urls=csv.reader(f.readlines())\n",
    "        self.url_dict={}\n",
    "        for i in urls:\n",
    "            #Saving the URLs into a dictionary will make it easy to access them later\n",
    "            self.url_dict[int(i[0])]=i[2]\n",
    "        \n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"Extracts the page that was visited and the visitor id\"\"\"\n",
    "        \n",
    "        line=line.strip().split(',')\n",
    "        page=line[1]\n",
    "        visitor=line[4]\n",
    "        #This is the \"Left Join\" logic that ensures that a row will be returned for\n",
    "        #every row in the master web page title\n",
    "        try:\n",
    "            url=self.url_dict[int(page)]\n",
    "        except KeyError:\n",
    "            url='NONE'\n",
    "        yield page,(visitor,url)\n",
    "   \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper,\n",
    "                )]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    LeftJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python leftjoin.py ms_logs.txt --file msft_urls.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Join Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Join returned rows = 98654\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from leftjoin import LeftJoin\n",
    "\n",
    "number_of_rows=0\n",
    "mr_job = LeftJoin(args=['ms_logs.txt','--file','msft_urls.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        number_of_rows+=1\n",
    "        \n",
    "print \"Left Join returned rows = \" + str(number_of_rows)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.2 - Right Hashside Join (memory-backed map-side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting RightJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile RightJoin.py\n",
    "#!/usr/bin/python\n",
    "## rightjoin.py\n",
    "## Author: James Gray\n",
    "## Description: perform right hashside join on web page master and web page visits\n",
    "\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class RightJoin(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        \"\"\"Load file of page URLs into memory\"\"\"\n",
    "        with open('msft_urls.txt','rb') as f:\n",
    "            urls=csv.reader(f.readlines())\n",
    "        self.url_dict={}\n",
    "        for i in urls:\n",
    "            #Saving the URLs into a dictionary will make it easy to access them later\n",
    "            #the second term here is a flag to see if we've emitted a record yet\n",
    "            self.url_dict[int(i[0])]=[i[2],0] \n",
    "        \n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"Extracts the page that was visited and the visitor id\"\"\"\n",
    "        line=line.strip().split(',')\n",
    "        page=line[1]\n",
    "        visitor=line[4]\n",
    "        \n",
    "        try:\n",
    "            url=self.url_dict[int(page)][0]\n",
    "            self.url_dict[int(page)][1]=1 #set flag to indicate we've emitted the record\n",
    "            yield page,(visitor,url)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    def mapper_final(self):\n",
    "        \"\"\"emit any records in the right table we haven't seen yet\"\"\"\n",
    "        for i in self.url_dict.iteritems():\n",
    "            if i[1][1]==0:\n",
    "                page=i[0]\n",
    "                url=i[1][0]\n",
    "                yield page,('NONE',url)\n",
    "   \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper,\n",
    "                mapper_final=self.mapper_final\n",
    "                )]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    RightJoin.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right Join Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right Join returned rows = 98704\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from RightJoin import RightJoin\n",
    "\n",
    "number_of_rows=0\n",
    "mr_job = RightJoin(args=['ms_logs.txt','--file','msft_urls.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        number_of_rows+=1\n",
    "        \n",
    "print \"Right Join returned rows = \" + str(number_of_rows)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.2 - Inner Hashside Join (memory-backed map-side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting InnerJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile InnerJoin.py\n",
    "#!/usr/bin/python\n",
    "## innerjoin.py\n",
    "## Author: James Gray\n",
    "## Description: perform inner hashside join on web page master and web page visits\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class InnerJoin(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        \"\"\"Load file of page URLs into memory\"\"\"\n",
    "        with open('msft_urls.txt','rb') as f:\n",
    "            urls=csv.reader(f.readlines())\n",
    "        self.url_dict={}\n",
    "        for i in urls:\n",
    "            #Saving the URLs into a dictionary will make it easy to access them later\n",
    "            #the second term here is a flag to see if we've emitted a record yet\n",
    "            self.url_dict[int(i[0])]=[i[2],0] \n",
    "        \n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"Extracts the page that was visited and the visitor id\"\"\"\n",
    "        line=line.strip().split(',')\n",
    "        page=line[1]\n",
    "        visitor=line[4]\n",
    "        #This is the \"Inner Join\" logic that emits a row for every record appearing in both \n",
    "        #tables\n",
    "        try:\n",
    "            url=self.url_dict[int(page)][0]\n",
    "            self.url_dict[int(page)][1]=1 #set flag to indicate we've emitted the record\n",
    "            yield page,(visitor,url)\n",
    "        except KeyError:\n",
    "            #Skip records that don't appear in both tables\n",
    "            pass\n",
    "        \n",
    "   \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper\n",
    "            )]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    InnerJoin.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Join returned rows = 98654\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from InnerJoin import InnerJoin\n",
    "\n",
    "number_of_rows=0\n",
    "mr_job = InnerJoin(args=['ms_logs.txt','--file','msft_urls.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        number_of_rows+=1\n",
    "        \n",
    "print \"Inner Join returned rows = \" + str(number_of_rows)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.3  EDA of Google n-grams dataset\n",
    "\n",
    "A large subset of the Google n-grams dataset https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "   https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "   s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "For HW 5.3-5.5, for the Google n-grams dataset unit test and regression test your code using the first 10 lines of the following file:\n",
    "\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "\n",
    "Once you are happy with your test results proceed to generating your results on the Google n-grams dataset. \n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "1. Longest 5-gram (number of characters)\n",
    "2. Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "3. 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "4. Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRJob Configuration File (store on local machine ~/.mrjob.conf)\n",
    "\n",
    "runners:\n",
    "  emr:\n",
    "    ec2_key_pair: w261   \n",
    "    ec2_key_pair_file: ~/.aws/w261.pem   \n",
    "    ssh_tunnel: true   \n",
    "    aws_region: us-east-1   \n",
    "    ec2_core_instance_type: m3.xlarge   \n",
    "    ec2_master_instance_type: m3.xlarge  \n",
    "    num_ec2_core_instances: 3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Amazon EMR Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/jamesgray/.mrjob.conf\n",
      "Using s3://mrjob-5a4b4386e2160458/tmp/ as our temp dir on S3\n",
      "Creating persistent cluster to run several jobs in...\n",
      "Creating temp directory /var/folders/ld/9wpyxfw13t7_pdv_0b8958x40000gn/T/no_script.jamesgray.20160619.143625.323507\n",
      "Copying local files to s3://mrjob-5a4b4386e2160458/tmp/no_script.jamesgray.20160619.143625.323507/files/...\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "j-1MTWMS2Y5VS2R\n"
     ]
    }
   ],
   "source": [
    "# create EMR Cluster\n",
    "\n",
    "!mrjob create-cluster --max-hours-idle 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\r\n",
      "A Biography of General George\t92\t90\t74\r\n",
      "A Case Study in Government\t102\t102\t78\r\n",
      "A Case Study of Female\t447\t447\t327\r\n",
      "A Case Study of Limited\t55\t55\t43\r\n",
      "A Child's Christmas in Wales\t1099\t1061\t866\r\n",
      "A Circumstantial Narrative of the\t62\t62\t50\r\n",
      "A City by the Sea\t62\t60\t49\r\n",
      "A Collection of Fairy Tales\t123\t117\t80\r\n",
      "A Collection of Forms of\t116\t103\t82\r\n",
      "A Commentary on his Apology\t110\t110\t69\r\n",
      "A Comparative Study of Juvenile\t68\t64\t44\r\n",
      "A Comparison of the Properties\t72\t72\t60\r\n",
      "A Conceptual Framework and the\t91\t91\t67\r\n",
      "A Conceptual Framework for Life\t49\t49\t40\r\n",
      "A Concise Bibliography of the\t145\t143\t122\r\n",
      "A Continuation of the Letters\t52\t51\t40\r\n",
      "A Critical Review and a\t197\t194\t155\r\n",
      "A Critique and a Guide\t42\t42\t42\r\n",
      "A Defence of the Royal\t153\t153\t120"
     ]
    }
   ],
   "source": [
    "!cat ngramtest.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.3 Part 1 - Longest 5-gram (Number of Characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing longestfivegram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile longestfivegram.py\n",
    "#!/usr/bin/python\n",
    "## longestfivegram.py\n",
    "## Author: James Gray\n",
    "## Description: calculate the 5-gram with the largest number of characters in the line\n",
    "\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class LongestNgram(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"Emit one record for each ngram length with its corresponding text \"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        ngram=line[0]\n",
    "        #We don't need keys here, since we want the overall max\n",
    "        yield None,(len(ngram),ngram)\n",
    "        \n",
    "    def reducer(self, _, ngram_and_length):\n",
    "        \"\"\"Return only the ngram with the max character length\"\"\"\n",
    "        yield None, max(ngram_and_length)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper\n",
    "                   #Recycle the reducer for the combiner as well\n",
    "                   ,combiner=self.reducer\n",
    "                    ,reducer=self.reducer\n",
    "                  )        \n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    LongestNgram.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest 5-gram Driver (Local Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, [33, 'A Circumstantial Narrative of the'])\n"
     ]
    }
   ],
   "source": [
    "#HW 5.3.A - Driver Function for Testing\n",
    "from longestfivegram import LongestNgram\n",
    "\n",
    "def run_53ngram():\n",
    "    mr_job = LongestNgram(args=['ngramtest.txt'])\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            print mr_job.parse_output_line(line)\n",
    "\n",
    "run_53ngram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest 5-gram - Run on Amazon EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/jamesgray/.mrjob.conf\n",
      "Using s3://mrjob-5a4b4386e2160458/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/ld/9wpyxfw13t7_pdv_0b8958x40000gn/T/longestfivegram.jamesgray.20160618.165331.680114\n",
      "Copying local files to s3://mrjob-5a4b4386e2160458/tmp/longestfivegram.jamesgray.20160618.165331.680114/files/...\n",
      "Adding our job to existing cluster j-3NHTTKYIYQAHF\n",
      "Waiting for step 1 of 1 (s-2Q83PWD5YS28W) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40456/cluster\n",
      "  RUNNING for 3.6s\n",
      "   100.0% complete\n",
      "  RUNNING for 36.2s\n",
      "     0.0% complete\n",
      "  RUNNING for 67.2s\n",
      "     5.1% complete\n",
      "  RUNNING for 98.5s\n",
      "     9.0% complete\n",
      "  RUNNING for 129.2s\n",
      "    11.1% complete\n",
      "  RUNNING for 160.2s\n",
      "    14.5% complete\n",
      "  RUNNING for 191.3s\n",
      "    17.5% complete\n",
      "  RUNNING for 222.6s\n",
      "    20.6% complete\n",
      "  RUNNING for 253.1s\n",
      "    23.7% complete\n",
      "  RUNNING for 284.1s\n",
      "    27.6% complete\n",
      "  RUNNING for 314.8s\n",
      "    30.5% complete\n",
      "  RUNNING for 345.9s\n",
      "    34.1% complete\n",
      "  RUNNING for 376.9s\n",
      "    37.3% complete\n",
      "  RUNNING for 408.2s\n",
      "    39.7% complete\n",
      "  RUNNING for 439.4s\n",
      "    42.8% complete\n",
      "  RUNNING for 470.6s\n",
      "    45.6% complete\n",
      "  RUNNING for 501.4s\n",
      "    48.2% complete\n",
      "  RUNNING for 533.1s\n",
      "    51.2% complete\n",
      "  RUNNING for 564.2s\n",
      "    53.8% complete\n",
      "  RUNNING for 600.1s\n",
      "    82.7% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-2Q83PWD5YS28W on ec2-54-165-245-217.compute-1.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-165-245-217.compute-1.amazonaws.com/mnt/var/log/hadoop/steps/s-2Q83PWD5YS28W/syslog.2016-06-18-16\n",
      "  Parsing step log: ssh://ec2-54-165-245-217.compute-1.amazonaws.com/mnt/var/log/hadoop/steps/s-2Q83PWD5YS28W/syslog\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=174\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=9044\n",
      "\t\tFILE: Number of bytes written=21003901\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=23450\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=190\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=2156069116\n",
      "\t\tS3: Number of bytes written=174\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=188\n",
      "\t\tLaunched map tasks=190\n",
      "\t\tLaunched reduce tasks=11\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11238132960\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5844150720\n",
      "\t\tTotal time spent by all map tasks (ms)=7804259\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=351191655\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2029219\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=182629710\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7804259\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2029219\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2947120\n",
      "\t\tCombine input records=58682266\n",
      "\t\tCombine output records=188\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=55808\n",
      "\t\tInput split bytes=23450\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=2331730493\n",
      "\t\tMap output materialized bytes=48307\n",
      "\t\tMap output records=58682266\n",
      "\t\tMerged Map outputs=2090\n",
      "\t\tPhysical memory (bytes) snapshot=100191006720\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=188\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=48307\n",
      "\t\tShuffled Maps =2090\n",
      "\t\tSpilled Records=376\n",
      "\t\tTotal committed heap usage (bytes)=125291200512\n",
      "\t\tVirtual memory (bytes) snapshot=409895063552\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-5a4b4386e2160458/tmp/longestfivegram.jamesgray.20160618.165331.680114/...\n",
      "Removing temp directory /var/folders/ld/9wpyxfw13t7_pdv_0b8958x40000gn/T/longestfivegram.jamesgray.20160618.165331.680114...\n",
      "Killing our SSH tunnel (pid 28208)\n"
     ]
    }
   ],
   "source": [
    "!python ./longestfivegram.py \\\n",
    "    -r emr s3://filtered-5grams  \\\n",
    "    --cluster-id j-3NHTTKYIYQAHF \\\n",
    "    --output-dir=s3://jamesgray-w261/longestfivegram \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy S3 Output to Local File System - Longest NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jamesgray-w261/longestfivegram/part-00001 to longestfivegram/part-00001\n",
      "download: s3://jamesgray-w261/longestfivegram/part-00000 to longestfivegram/part-00000\n",
      "download: s3://jamesgray-w261/longestfivegram/part-00006 to longestfivegram/part-00006\n",
      "download: s3://jamesgray-w261/longestfivegram/part-00003 to longestfivegram/part-00003\n",
      "download: s3://jamesgray-w261/longestfivegram/part-00004 to longestfivegram/part-00004\n",
      "download: s3://jamesgray-w261/longestfivegram/part-00005 to longestfivegram/part-00005\n",
      "download: s3://jamesgray-w261/longestfivegram/part-00007 to longestfivegram/part-00007\n",
      "download: s3://jamesgray-w261/longestfivegram/part-00008 to longestfivegram/part-00008\n",
      "download: s3://jamesgray-w261/longestfivegram/part-00009 to longestfivegram/part-00009\n",
      "download: s3://jamesgray-w261/longestfivegram/part-00010 to longestfivegram/part-00010\n",
      "download: s3://jamesgray-w261/longestfivegram/part-00002 to longestfivegram/part-00002\n",
      "download: s3://jamesgray-w261/longestfivegram/_SUCCESS to longestfivegram/_SUCCESS\n",
      "LONGEST FIVE NGRAM:\n",
      "null\t[159, \"ROPLEZIMPREDASTRODONBRASLPKLSON YHROACLMPARCHEYXMMIOUDAVESAURUS PIOFPILOCOWERSURUASOGETSESNEGCP TYRAVOPSIFENGOQUAPIALLOBOSKENUO OWINFUYAIOKENECKSASXHYILPOYNUAT\"]\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://jamesgray-w261/longestfivegram ./longestfivegram \n",
    "!echo \"LONGEST FIVE NGRAM:\"\n",
    "!cat ./longestfivegram/part-* | sort -k2nr | head -1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.3 Part 2 - Top 10 most frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting toptenwords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile toptenwords.py\n",
    "#!/usr/bin/python\n",
    "## toptenwords.py\n",
    "## Author: James Gray\n",
    "## Description: calculate the top ten words across the n-gram corpus\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from mrjob import conf\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class TopTenWords(MRJob):\n",
    "            \n",
    "    def mapper(self, _, line):\n",
    "        counts = {}\n",
    "        line.strip()\n",
    "        #Parse fields from each line\n",
    "        [ngram,count,pages,books] = re.split(\"\\t\",line)\n",
    "        count = int(count)\n",
    "        words = re.split(\" \",ngram)\n",
    "        for word in words:\n",
    "            # set words to lowercase\n",
    "            counts.setdefault(word.lower(),0)\n",
    "            counts[word.lower()] += count\n",
    "        for word in counts.keys():\n",
    "            yield word,counts[word]\n",
    "    \n",
    "    def combiner(self,word,count):\n",
    "        yield word,sum(count)\n",
    "            \n",
    "    def reducer(self,word,count):\n",
    "        yield word,sum(count)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper\n",
    "                   ,combiner=self.combiner\n",
    "                    ,reducer=self.reducer\n",
    "                  )\n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    TopTenWords.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Ten Words Driver (Local Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 3435)\n",
      "('and', 330)\n",
      "('apology', 110)\n",
      "('bibliography', 145)\n",
      "('bill', 59)\n",
      "('biography', 92)\n",
      "('by', 62)\n",
      "('case', 604)\n",
      "(\"child's\", 1099)\n",
      "('christmas', 1099)\n",
      "('circumstantial', 62)\n",
      "('city', 62)\n",
      "('collection', 239)\n",
      "('commentary', 110)\n",
      "('comparative', 68)\n",
      "('comparison', 72)\n",
      "('conceptual', 140)\n",
      "('concise', 145)\n",
      "('continuation', 52)\n",
      "('critical', 197)\n",
      "('critique', 42)\n",
      "('defence', 153)\n",
      "('establishing', 59)\n",
      "('fairy', 123)\n",
      "('female', 447)\n",
      "('for', 108)\n",
      "('forms', 116)\n",
      "('framework', 140)\n",
      "('general', 92)\n",
      "('george', 92)\n",
      "('government', 102)\n",
      "('guide', 42)\n",
      "('his', 110)\n",
      "('in', 1201)\n",
      "('juvenile', 68)\n",
      "('letters', 52)\n",
      "('life', 49)\n",
      "('limited', 55)\n",
      "('narrative', 62)\n",
      "('of', 1501)\n",
      "('on', 110)\n",
      "('properties', 72)\n",
      "('religious', 59)\n",
      "('review', 197)\n",
      "('royal', 153)\n",
      "('sea', 62)\n",
      "('study', 672)\n",
      "('tales', 123)\n",
      "('the', 637)\n",
      "('wales', 1099)\n"
     ]
    }
   ],
   "source": [
    "from toptenwords import TopTenWords\n",
    "\n",
    "def run_topten():\n",
    "    mr_job = TopTenWords(args=['ngramtest.txt'])\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            print mr_job.parse_output_line(line)\n",
    "\n",
    "run_topten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Ten Words - Amazon EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/jamesgray/.mrjob.conf\n",
      "Using s3://mrjob-5a4b4386e2160458/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/ld/9wpyxfw13t7_pdv_0b8958x40000gn/T/toptenwords.jamesgray.20160618.175514.477196\n",
      "Copying local files to s3://mrjob-5a4b4386e2160458/tmp/toptenwords.jamesgray.20160618.175514.477196/files/...\n",
      "Adding our job to existing cluster j-3NHTTKYIYQAHF\n",
      "Waiting for step 1 of 1 (s-34LL837RG0FY2) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40456/cluster\n",
      "  RUNNING for 18.2s\n",
      "   100.0% complete\n",
      "  RUNNING for 50.2s\n",
      "     5.0% complete\n",
      "  RUNNING for 81.0s\n",
      "     5.6% complete\n",
      "  RUNNING for 112.4s\n",
      "     7.1% complete\n",
      "  RUNNING for 143.6s\n",
      "     8.4% complete\n",
      "  RUNNING for 174.9s\n",
      "     8.9% complete\n",
      "  RUNNING for 205.5s\n",
      "    10.6% complete\n",
      "  RUNNING for 237.2s\n",
      "    11.6% complete\n",
      "  RUNNING for 268.0s\n",
      "    12.7% complete\n",
      "  RUNNING for 299.3s\n",
      "    14.0% complete\n",
      "  RUNNING for 330.0s\n",
      "    15.0% complete\n",
      "  RUNNING for 360.9s\n",
      "    16.5% complete\n",
      "  RUNNING for 392.0s\n",
      "    17.5% complete\n",
      "  RUNNING for 423.2s\n",
      "    18.7% complete\n",
      "  RUNNING for 453.7s\n",
      "    20.1% complete\n",
      "  RUNNING for 484.2s\n",
      "    21.9% complete\n",
      "  RUNNING for 515.4s\n",
      "    22.3% complete\n",
      "  RUNNING for 546.4s\n",
      "    24.1% complete\n",
      "  RUNNING for 577.3s\n",
      "    25.3% complete\n",
      "  RUNNING for 608.1s\n",
      "    26.6% complete\n",
      "  RUNNING for 639.0s\n",
      "    28.5% complete\n",
      "  RUNNING for 670.6s\n",
      "    29.3% complete\n",
      "  RUNNING for 701.1s\n",
      "    31.4% complete\n",
      "  RUNNING for 732.0s\n",
      "    32.4% complete\n",
      "  RUNNING for 762.9s\n",
      "    33.6% complete\n",
      "  RUNNING for 794.7s\n",
      "    34.7% complete\n",
      "  RUNNING for 825.4s\n",
      "    35.8% complete\n",
      "  RUNNING for 856.4s\n",
      "    36.8% complete\n",
      "  RUNNING for 887.3s\n",
      "    38.2% complete\n",
      "  RUNNING for 919.0s\n",
      "    39.0% complete\n",
      "  RUNNING for 949.8s\n",
      "    40.3% complete\n",
      "  RUNNING for 980.9s\n",
      "    41.2% complete\n",
      "  RUNNING for 1011.7s\n",
      "    42.6% complete\n",
      "  RUNNING for 1042.3s\n",
      "    43.6% complete\n",
      "  RUNNING for 1073.6s\n",
      "    44.7% complete\n",
      "  RUNNING for 1104.1s\n",
      "    45.5% complete\n",
      "  RUNNING for 1135.1s\n",
      "    47.0% complete\n",
      "  RUNNING for 1167.0s\n",
      "    48.0% complete\n",
      "  RUNNING for 1197.3s\n",
      "    49.2% complete\n",
      "  RUNNING for 1228.8s\n",
      "    49.9% complete\n",
      "  RUNNING for 1259.5s\n",
      "    51.4% complete\n",
      "  RUNNING for 1291.3s\n",
      "    52.5% complete\n",
      "  RUNNING for 1322.0s\n",
      "    53.4% complete\n",
      "  RUNNING for 1353.8s\n",
      "    54.2% complete\n",
      "  RUNNING for 1384.6s\n",
      "    58.8% complete\n",
      "  RUNNING for 1416.3s\n",
      "    60.7% complete\n",
      "  RUNNING for 1447.1s\n",
      "   100.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-34LL837RG0FY2 on ec2-54-165-245-217.compute-1.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-165-245-217.compute-1.amazonaws.com/mnt/var/log/hadoop/steps/s-34LL837RG0FY2/syslog.2016-06-18-17\n",
      "  Parsing step log: ssh://ec2-54-165-245-217.compute-1.amazonaws.com/mnt/var/log/hadoop/steps/s-34LL837RG0FY2/syslog\n",
      "Counters: 57\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4158739\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=39469420\n",
      "\t\tFILE: Number of bytes written=146996633\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=23450\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=190\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=2156069116\n",
      "\t\tS3: Number of bytes written=4158739\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=189\n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=191\n",
      "\t\tLaunched reduce tasks=12\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=28796991840\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=14732625600\n",
      "\t\tTotal time spent by all map tasks (ms)=19997911\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=899905995\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5115495\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=460394550\n",
      "\t\tTotal vcore-seconds taken by all map tasks=19997911\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5115495\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=9645850\n",
      "\t\tCombine input records=288622387\n",
      "\t\tCombine output records=6822745\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=62383\n",
      "\t\tInput split bytes=23450\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=3095339740\n",
      "\t\tMap output materialized bytes=86588497\n",
      "\t\tMap output records=288622387\n",
      "\t\tMerged Map outputs=2090\n",
      "\t\tPhysical memory (bytes) snapshot=107926491136\n",
      "\t\tReduce input groups=269339\n",
      "\t\tReduce input records=6822745\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=86588497\n",
      "\t\tShuffled Maps =2090\n",
      "\t\tSpilled Records=13645490\n",
      "\t\tTotal committed heap usage (bytes)=125604724736\n",
      "\t\tVirtual memory (bytes) snapshot=409697017856\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-5a4b4386e2160458/tmp/toptenwords.jamesgray.20160618.175514.477196/...\n",
      "Removing temp directory /var/folders/ld/9wpyxfw13t7_pdv_0b8958x40000gn/T/toptenwords.jamesgray.20160618.175514.477196...\n",
      "Killing our SSH tunnel (pid 28833)\n"
     ]
    }
   ],
   "source": [
    "!python ./toptenwords.py \\\n",
    "    -r emr s3://filtered-5grams  \\\n",
    "    --cluster-id j-3NHTTKYIYQAHF \\\n",
    "    --output-dir=s3://jamesgray-w261/toptenwords \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Output to Local File System - Top Ten Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jamesgray-w261/toptenwords/_SUCCESS to toptenwords/_SUCCESS\n",
      "download: s3://jamesgray-w261/toptenwords/part-00007 to toptenwords/part-00007\n",
      "download: s3://jamesgray-w261/toptenwords/part-00004 to toptenwords/part-00004\n",
      "download: s3://jamesgray-w261/toptenwords/part-00006 to toptenwords/part-00006\n",
      "download: s3://jamesgray-w261/toptenwords/part-00002 to toptenwords/part-00002\n",
      "download: s3://jamesgray-w261/toptenwords/part-00000 to toptenwords/part-00000\n",
      "download: s3://jamesgray-w261/toptenwords/part-00005 to toptenwords/part-00005\n",
      "download: s3://jamesgray-w261/toptenwords/part-00010 to toptenwords/part-00010\n",
      "download: s3://jamesgray-w261/toptenwords/part-00008 to toptenwords/part-00008\n",
      "download: s3://jamesgray-w261/toptenwords/part-00001 to toptenwords/part-00001\n",
      "download: s3://jamesgray-w261/toptenwords/part-00009 to toptenwords/part-00009\n",
      "download: s3://jamesgray-w261/toptenwords/part-00003 to toptenwords/part-00003\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n",
      "TOP TEN WORDS\n",
      "\"the\"\t5490815394\n",
      "\"of\"\t3698583299\n",
      "\"to\"\t2227866570\n",
      "\"in\"\t1421312776\n",
      "\"a\"\t1361123022\n",
      "\"and\"\t1149577477\n",
      "\"that\"\t802921147\n",
      "\"is\"\t758328796\n",
      "\"be\"\t688707130\n",
      "\"as\"\t492170314\n",
      "cat: stdout: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://jamesgray-w261/toptenwords ./toptenwords \n",
    "!cat ./toptenwords/part-* | sort -k2nr | head -10000 > ./toptenwords.txt\n",
    "!echo \"TOP TEN WORDS\"\n",
    "!cat ./toptenwords.txt | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.3 Part 3 - 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting density.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile density.py\n",
    "#!/usr/bin/python\n",
    "## density.py\n",
    "## Author: James Gray\n",
    "## Description: calculate 20 most/least densely appearing words\n",
    "\n",
    "from __future__ import division\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class Density(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"Emit one record per word with count and page count \"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        ngram=line[0]\n",
    "        count=line[1]\n",
    "        page_count=line[2]\n",
    "        for word in ngram.split(' '):\n",
    "            yield word,(count,page_count)\n",
    "            \n",
    "    def combiner(self,word,count):\n",
    "        \"\"\"Aggregate intermediate word counts and page counts\"\"\"\n",
    "        word_count=0\n",
    "        page_count=0\n",
    "        for record in count:\n",
    "            word_count+=int(record[0])\n",
    "            page_count+=int(record[1])\n",
    "        yield word,(word_count,page_count)\n",
    "                    \n",
    "    def reducer(self,word,count):\n",
    "        \"\"\"Final aggregation of word counts and page counts, divided for relative frequency\"\"\"\n",
    "        word_count=0\n",
    "        page_count=0\n",
    "        for record in count:\n",
    "            word_count+=int(record[0])\n",
    "            page_count+=int(record[1])\n",
    "        yield word,word_count/page_count\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper\n",
    "                   ,combiner=self.combiner\n",
    "                    ,reducer=self.reducer\n",
    "                  )        \n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Density.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Driver - Local Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A', 1.02272)\n",
      "('Apology', 1.0)\n",
      "('BILL', 1.0)\n",
      "('Bibliography', 1.013986013986014)\n",
      "('Biography', 1.0222222222222221)\n",
      "('Case', 1.0)\n",
      "(\"Child's\", 1.0358152686145146)\n",
      "('Christmas', 1.0358152686145146)\n",
      "('Circumstantial', 1.0)\n",
      "('City', 1.0333333333333334)\n",
      "('Collection', 1.0863636363636364)\n",
      "('Commentary', 1.0)\n",
      "('Comparative', 1.0625)\n",
      "('Comparison', 1.0)\n",
      "('Conceptual', 1.0)\n",
      "('Concise', 1.013986013986014)\n",
      "('Continuation', 1.0196078431372548)\n",
      "('Critical', 1.0154639175257731)\n",
      "('Critique', 1.0)\n",
      "('Defence', 1.0)\n",
      "('ESTABLISHING', 1.0)\n",
      "('FOR', 1.0)\n",
      "('Fairy', 1.0512820512820513)\n",
      "('Female', 1.0)\n",
      "('Forms', 1.1262135922330097)\n",
      "('Framework', 1.0)\n",
      "('General', 1.0222222222222221)\n",
      "('George', 1.0222222222222221)\n",
      "('Government', 1.0)\n",
      "('Guide', 1.0)\n",
      "('Juvenile', 1.0625)\n",
      "('Letters', 1.0196078431372548)\n",
      "('Life', 1.0)\n",
      "('Limited', 1.0)\n",
      "('Narrative', 1.0)\n",
      "('Properties', 1.0)\n",
      "('RELIGIOUS', 1.0)\n",
      "('Review', 1.0154639175257731)\n",
      "('Royal', 1.0)\n",
      "('Sea', 1.0333333333333334)\n",
      "('Study', 1.0059880239520957)\n",
      "('Tales', 1.0512820512820513)\n",
      "('Wales', 1.0358152686145146)\n",
      "('a', 1.0127118644067796)\n",
      "('and', 1.0091743119266054)\n",
      "('by', 1.0333333333333334)\n",
      "('for', 1.0)\n",
      "('his', 1.0)\n",
      "('in', 1.0326741186586414)\n",
      "('of', 1.0280821917808218)\n",
      "('on', 1.0)\n",
      "('the', 1.0079113924050633)\n"
     ]
    }
   ],
   "source": [
    "from density import Density\n",
    "\n",
    "def run_density():\n",
    "    mr_job = Density(args=['ngramtest.txt'])\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            print mr_job.parse_output_line(line)\n",
    "\n",
    "run_density()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density - Amazon EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/jamesgray/.mrjob.conf\n",
      "Using s3://mrjob-5a4b4386e2160458/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/ld/9wpyxfw13t7_pdv_0b8958x40000gn/T/density.jamesgray.20160618.182345.829166\n",
      "Copying local files to s3://mrjob-5a4b4386e2160458/tmp/density.jamesgray.20160618.182345.829166/files/...\n",
      "Adding our job to existing cluster j-3NHTTKYIYQAHF\n",
      "Waiting for step 1 of 1 (s-1RUN0XE45HORO) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40456/cluster\n",
      "  RUNNING for 9.9s\n",
      "   100.0% complete\n",
      "  RUNNING for 42.5s\n",
      "     5.0% complete\n",
      "  RUNNING for 73.8s\n",
      "     5.2% complete\n",
      "  RUNNING for 104.8s\n",
      "     6.9% complete\n",
      "  RUNNING for 135.4s\n",
      "     8.3% complete\n",
      "  RUNNING for 166.7s\n",
      "     8.6% complete\n",
      "  RUNNING for 198.0s\n",
      "     9.1% complete\n",
      "  RUNNING for 229.6s\n",
      "    10.9% complete\n",
      "  RUNNING for 260.0s\n",
      "    12.2% complete\n",
      "  RUNNING for 291.2s\n",
      "    13.5% complete\n",
      "  RUNNING for 321.7s\n",
      "    14.2% complete\n",
      "  RUNNING for 352.6s\n",
      "    15.2% complete\n",
      "  RUNNING for 383.8s\n",
      "    16.6% complete\n",
      "  RUNNING for 415.5s\n",
      "    18.1% complete\n",
      "  RUNNING for 446.1s\n",
      "    19.0% complete\n",
      "  RUNNING for 477.7s\n",
      "    20.2% complete\n",
      "  RUNNING for 508.7s\n",
      "    21.9% complete\n",
      "  RUNNING for 539.5s\n",
      "    22.6% complete\n",
      "  RUNNING for 570.5s\n",
      "    24.1% complete\n",
      "  RUNNING for 601.4s\n",
      "    25.3% complete\n",
      "  RUNNING for 632.2s\n",
      "    26.2% complete\n",
      "  RUNNING for 663.2s\n",
      "    28.1% complete\n",
      "  RUNNING for 693.8s\n",
      "    29.2% complete\n",
      "  RUNNING for 725.5s\n",
      "    30.6% complete\n",
      "  RUNNING for 756.6s\n",
      "    32.2% complete\n",
      "  RUNNING for 787.9s\n",
      "    32.9% complete\n",
      "  RUNNING for 818.3s\n",
      "    34.2% complete\n",
      "  RUNNING for 849.4s\n",
      "    35.4% complete\n",
      "  RUNNING for 880.5s\n",
      "    36.3% complete\n",
      "  RUNNING for 912.1s\n",
      "    37.0% complete\n",
      "  RUNNING for 943.3s\n",
      "    38.6% complete\n",
      "  RUNNING for 974.4s\n",
      "    39.4% complete\n",
      "  RUNNING for 1005.3s\n",
      "    40.6% complete\n",
      "  RUNNING for 1036.4s\n",
      "    41.4% complete\n",
      "  RUNNING for 1066.9s\n",
      "    42.6% complete\n",
      "  RUNNING for 1098.4s\n",
      "    43.9% complete\n",
      "  RUNNING for 1129.1s\n",
      "    44.8% complete\n",
      "  RUNNING for 1159.6s\n",
      "    45.5% complete\n",
      "  RUNNING for 1190.8s\n",
      "    47.0% complete\n",
      "  RUNNING for 1221.9s\n",
      "    48.0% complete\n",
      "  RUNNING for 1252.3s\n",
      "    49.0% complete\n",
      "  RUNNING for 1283.6s\n",
      "    49.9% complete\n",
      "  RUNNING for 1314.9s\n",
      "    50.8% complete\n",
      "  RUNNING for 1346.0s\n",
      "    52.3% complete\n",
      "  RUNNING for 1376.6s\n",
      "    53.2% complete\n",
      "  RUNNING for 1408.1s\n",
      "    54.0% complete\n",
      "  RUNNING for 1438.7s\n",
      "    57.1% complete\n",
      "  RUNNING for 1469.3s\n",
      "    60.4% complete\n",
      "  RUNNING for 1499.8s\n",
      "    80.7% complete\n",
      "  RUNNING for 1531.0s\n",
      "   100.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-1RUN0XE45HORO on ec2-54-165-245-217.compute-1.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-165-245-217.compute-1.amazonaws.com/mnt/var/log/hadoop/steps/s-1RUN0XE45HORO/syslog\n",
      "Counters: 56\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7772250\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=72132421\n",
      "\t\tFILE: Number of bytes written=223925136\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=23450\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=190\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=2156069116\n",
      "\t\tS3: Number of bytes written=7772250\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=188\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=190\n",
      "\t\tLaunched reduce tasks=12\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=30113605440\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=15571244160\n",
      "\t\tTotal time spent by all map tasks (ms)=20912226\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=941050170\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5406682\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=486601380\n",
      "\t\tTotal vcore-seconds taken by all map tasks=20912226\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5406682\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=10344970\n",
      "\t\tCombine input records=293411330\n",
      "\t\tCombine output records=7922043\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=61208\n",
      "\t\tInput split bytes=23450\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=6170741820\n",
      "\t\tMap output materialized bytes=130862175\n",
      "\t\tMap output records=293411330\n",
      "\t\tMerged Map outputs=2090\n",
      "\t\tPhysical memory (bytes) snapshot=107584503808\n",
      "\t\tReduce input groups=343019\n",
      "\t\tReduce input records=7922043\n",
      "\t\tReduce output records=343019\n",
      "\t\tReduce shuffle bytes=130862175\n",
      "\t\tShuffled Maps =2090\n",
      "\t\tSpilled Records=15844086\n",
      "\t\tTotal committed heap usage (bytes)=125518217216\n",
      "\t\tVirtual memory (bytes) snapshot=409888284672\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-5a4b4386e2160458/tmp/density.jamesgray.20160618.182345.829166/...\n",
      "Removing temp directory /var/folders/ld/9wpyxfw13t7_pdv_0b8958x40000gn/T/density.jamesgray.20160618.182345.829166...\n",
      "Killing our SSH tunnel (pid 28953)\n"
     ]
    }
   ],
   "source": [
    "!python ./density.py \\\n",
    "    -r emr s3://filtered-5grams  \\\n",
    "    --cluster-id j-3NHTTKYIYQAHF \\\n",
    "    --output-dir=s3://jamesgray-w261/density \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Output to Local File System - Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jamesgray-w261/density/_SUCCESS to density/_SUCCESS\n",
      "download: s3://jamesgray-w261/density/part-00005 to density/part-00005\n",
      "download: s3://jamesgray-w261/density/part-00004 to density/part-00004\n",
      "download: s3://jamesgray-w261/density/part-00002 to density/part-00002\n",
      "download: s3://jamesgray-w261/density/part-00000 to density/part-00000\n",
      "download: s3://jamesgray-w261/density/part-00001 to density/part-00001\n",
      "download: s3://jamesgray-w261/density/part-00010 to density/part-00010\n",
      "download: s3://jamesgray-w261/density/part-00008 to density/part-00008\n",
      "download: s3://jamesgray-w261/density/part-00006 to density/part-00006\n",
      "download: s3://jamesgray-w261/density/part-00003 to density/part-00003\n",
      "download: s3://jamesgray-w261/density/part-00009 to density/part-00009\n",
      "download: s3://jamesgray-w261/density/part-00007 to density/part-00007\n",
      "TOP 20 HIGH DENSITY WORDS\n",
      "\"xxxx\"\t11.557291666666666\n",
      "\"NA\"\t10.161726044782885\n",
      "\"blah\"\t8.074159907300116\n",
      "\"nnn\"\t7.533333333333333\n",
      "\"nd\"\t6.561143644505684\n",
      "\"ND\"\t5.40736428467472\n",
      "\"oooooooooooooooo\"\t4.921875\n",
      "\"PIC\"\t4.7272727272727275\n",
      "\"llll\"\t4.511627906976744\n",
      "\"LUTHER\"\t4.349498327759197\n",
      "\"oooooo\"\t4.207237859573151\n",
      "\"NN\"\t4.0908402725208175\n",
      "\"ooooo\"\t3.9492846924177396\n",
      "\"OOOOOO\"\t3.9313725490196076\n",
      "\"IIII\"\t3.7877030162412995\n",
      "\"lillelu\"\t3.7624521072796937\n",
      "\"OOOOO\"\t3.6570701447431206\n",
      "\"Sc\"\t3.6065625\n",
      "\"Madarassy\"\t3.576923076923077\n",
      "\"Pfeffermann\"\t3.576923076923077\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n",
      "\n",
      "BOTTOM 20 LOWEST DENSITY WORDS\n",
      "\"zusammen\"\t1.0\n",
      "\"zusammengenommen\"\t1.0\n",
      "\"zusammengetroffen\"\t1.0\n",
      "\"zwangloser\"\t1.0\n",
      "\"zwanziger\"\t1.0\n",
      "\"zweier\"\t1.0\n",
      "\"zweifache\"\t1.0\n",
      "\"zweigt\"\t1.0\n",
      "\"zwingst\"\t1.0\n",
      "\"zwischenstaatlicher\"\t1.0\n",
      "\"zwitterionic\"\t1.0\n",
      "\"zydeco\"\t1.0\n",
      "\"zygomaticofacial\"\t1.0\n",
      "\"zygomaticotemporal\"\t1.0\n",
      "\"zygosity\"\t1.0\n",
      "\"zylindrischen\"\t1.0\n",
      "\"zymogens\"\t1.0\n",
      "\"zymophore\"\t1.0\n",
      "\"zymosan\"\t1.0\n",
      "\"zymosis\"\t1.0\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://jamesgray-w261/density ./density\n",
    "!echo \"TOP 20 HIGH DENSITY WORDS\"\n",
    "!cat ./density/part-* | sort -k2nr | head -20 \n",
    "!echo \"\"\n",
    "!echo \"BOTTOM 20 LOWEST DENSITY WORDS\"\n",
    "!cat ./density/part-* | sort -k2nr | tail -20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.3 Part 4 - Distribution of 5-gram sizes (character length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing distribution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile distribution.py\n",
    "#HW 5.3.D - Ngram Distribution MRJob Definition\n",
    "from __future__ import division\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class Distribution(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.count=0\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"Emit records with ngrams and size\"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        ngram=line[0] #The text of the ngram\n",
    "        size=len(ngram)\n",
    "        ngram_count=int(line[1]) #The count of the ngram\n",
    "        self.count+=ngram_count #Add the count to the running total of counts\n",
    "        yield size,ngram_count #Yield the ngram and its count\n",
    "    \n",
    "    def mapper_final(self):\n",
    "        \"\"\"We needed this for the original statement of the problem, which\n",
    "        required relative frequencies.  Though we've left the step in place, \n",
    "        the result is no longer used.\"\"\"\n",
    "        yield '*count',self.count #Yield the total for order-inversion\n",
    "        \n",
    "    def reducer_init(self):\n",
    "        self.total_count=None\n",
    "            \n",
    "    def reducer(self,size,ngram_count):\n",
    "        total=sum(ngram_count)\n",
    "        overall_total=None\n",
    "        #Capture the totals for a relative frequency calcuation (no longer used)\n",
    "        if size=='*count': \n",
    "            overall_total=total\n",
    "            self.total_count=total\n",
    "        else:\n",
    "            #Yield the character length and the number of ngrams with that length\n",
    "            #(relative freq. calculation is commented out)\n",
    "            yield size,(total)#, total/self.total_count)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper\n",
    "                ,mapper_final=self.mapper_final\n",
    "                ,reducer_init=self.reducer_init\n",
    "                ,reducer=self.reducer\n",
    "                  )        \n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Distribution.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Driver (Local Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 62)\n",
      "(22, 642)\n",
      "(23, 252)\n",
      "(24, 116)\n",
      "(26, 102)\n",
      "(27, 233)\n",
      "(28, 1099)\n",
      "(29, 289)\n",
      "(30, 163)\n",
      "(31, 117)\n",
      "(33, 121)\n"
     ]
    }
   ],
   "source": [
    "from distribution import Distribution\n",
    "\n",
    "def run_distribution():\n",
    "    mr_job = Distribution(args=['ngramtest.txt'])\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            print mr_job.parse_output_line(line)\n",
    "\n",
    "run_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution - Amazon EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/jamesgray/.mrjob.conf\n",
      "Using s3://mrjob-5a4b4386e2160458/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/ld/9wpyxfw13t7_pdv_0b8958x40000gn/T/distribution.jamesgray.20160618.185533.609256\n",
      "Copying local files to s3://mrjob-5a4b4386e2160458/tmp/distribution.jamesgray.20160618.185533.609256/files/...\n",
      "Adding our job to existing cluster j-3NHTTKYIYQAHF\n",
      "Waiting for step 1 of 1 (s-2JEP66J97PZTJ) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40456/cluster\n",
      "  RUNNING for 20.8s\n",
      "   100.0% complete\n",
      "  RUNNING for 53.0s\n",
      "     5.0% complete\n",
      "  RUNNING for 83.9s\n",
      "     7.6% complete\n",
      "  RUNNING for 115.1s\n",
      "    10.8% complete\n",
      "  RUNNING for 145.7s\n",
      "    14.9% complete\n",
      "  RUNNING for 177.4s\n",
      "    18.5% complete\n",
      "  RUNNING for 207.8s\n",
      "    22.4% complete\n",
      "  RUNNING for 239.2s\n",
      "    26.6% complete\n",
      "  RUNNING for 269.8s\n",
      "    29.9% complete\n",
      "  RUNNING for 300.8s\n",
      "    34.2% complete\n",
      "  RUNNING for 331.9s\n",
      "    38.0% complete\n",
      "  RUNNING for 363.1s\n",
      "    41.5% complete\n",
      "  RUNNING for 393.5s\n",
      "    44.5% complete\n",
      "  RUNNING for 425.1s\n",
      "    47.5% complete\n",
      "  RUNNING for 456.1s\n",
      "    51.1% complete\n",
      "  RUNNING for 487.7s\n",
      "    54.6% complete\n",
      "  RUNNING for 518.2s\n",
      "    80.0% complete\n",
      "  RUNNING for 549.6s\n",
      "    80.4% complete\n",
      "  RUNNING for 580.6s\n",
      "    88.2% complete\n",
      "  RUNNING for 612.1s\n",
      "   100.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-2JEP66J97PZTJ on ec2-54-165-245-217.compute-1.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-165-245-217.compute-1.amazonaws.com/mnt/var/log/hadoop/steps/s-2JEP66J97PZTJ/syslog.2016-06-18-18\n",
      "  Parsing step log: ssh://ec2-54-165-245-217.compute-1.amazonaws.com/mnt/var/log/hadoop/steps/s-2JEP66J97PZTJ/syslog\n",
      "Counters: 57\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=780\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=155799434\n",
      "\t\tFILE: Number of bytes written=333805840\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=23450\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=190\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=2156069116\n",
      "\t\tS3: Number of bytes written=780\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=189\n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=2\n",
      "\t\tLaunched map tasks=191\n",
      "\t\tLaunched reduce tasks=13\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=9404161920\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=7183428480\n",
      "\t\tTotal time spent by all map tasks (ms)=6530668\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=293880060\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2494246\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=224482140\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6530668\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2494246\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3020190\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=56048\n",
      "\t\tInput split bytes=23450\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=372259274\n",
      "\t\tMap output materialized bytes=157142200\n",
      "\t\tMap output records=58682456\n",
      "\t\tMerged Map outputs=2090\n",
      "\t\tPhysical memory (bytes) snapshot=100210114560\n",
      "\t\tReduce input groups=81\n",
      "\t\tReduce input records=58682456\n",
      "\t\tReduce output records=80\n",
      "\t\tReduce shuffle bytes=157142200\n",
      "\t\tShuffled Maps =2090\n",
      "\t\tSpilled Records=117364912\n",
      "\t\tTotal committed heap usage (bytes)=124011937792\n",
      "\t\tVirtual memory (bytes) snapshot=409752711168\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-5a4b4386e2160458/tmp/distribution.jamesgray.20160618.185533.609256/...\n",
      "Removing temp directory /var/folders/ld/9wpyxfw13t7_pdv_0b8958x40000gn/T/distribution.jamesgray.20160618.185533.609256...\n",
      "Killing our SSH tunnel (pid 29104)\n"
     ]
    }
   ],
   "source": [
    "!python ./distribution.py \\\n",
    "    -r emr s3://filtered-5grams  \\\n",
    "    --cluster-id j-3NHTTKYIYQAHF \\\n",
    "    --output-dir=s3://jamesgray-w261/distribution \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Output to Local File System & Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jamesgray-w261/distribution/part-00005 to distribution/part-00005\n",
      "download: s3://jamesgray-w261/distribution/part-00008 to distribution/part-00008\n",
      "download: s3://jamesgray-w261/distribution/part-00003 to distribution/part-00003\n",
      "download: s3://jamesgray-w261/distribution/part-00006 to distribution/part-00006\n",
      "download: s3://jamesgray-w261/distribution/part-00004 to distribution/part-00004\n",
      "download: s3://jamesgray-w261/distribution/part-00000 to distribution/part-00000\n",
      "download: s3://jamesgray-w261/distribution/part-00007 to distribution/part-00007\n",
      "download: s3://jamesgray-w261/distribution/part-00001 to distribution/part-00001\n",
      "download: s3://jamesgray-w261/distribution/part-00002 to distribution/part-00002\n",
      "download: s3://jamesgray-w261/distribution/part-00009 to distribution/part-00009\n",
      "download: s3://jamesgray-w261/distribution/_SUCCESS to distribution/_SUCCESS\n",
      "download: s3://jamesgray-w261/distribution/part-00010 to distribution/part-00010\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://jamesgray-w261/distribution ./distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'part-00000'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5f6a939c3dfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mrun_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-5f6a939c3dfb>\u001b[0m in \u001b[0;36mrun_plot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'part'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m#Load results from each output file we downloaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'part-00000'"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_plot():\n",
    "    lengths=[]\n",
    "    totals=[]\n",
    "    for i in os.listdir('./distribution'):\n",
    "        if i.startswith('part'):\n",
    "            #Load results from each output file we downloaded\n",
    "            with open(i) as f:\n",
    "                for line in f.readlines():\n",
    "                    [length,total]=line.strip().split('\\t')\n",
    "                    #Save lengths and totals into separate vectors for easy plotting\n",
    "                    lengths.append(int(length))\n",
    "                    totals.append(int(total))\n",
    "\n",
    "    fig, chart = plt.subplots()\n",
    "    #We already know the bar heights, so we can plot them directly rather than making a histogram\n",
    "    chart.bar(lengths,totals) \n",
    "    chart.set_ylabel('Count')\n",
    "    chart.set_xlabel('N-gram length (in characters)')\n",
    "    chart.set_title('Distribution of n-gram lengths')\n",
    "\n",
    "    fig = plt.gcf()\n",
    "\n",
    "run_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.3.1 OPTIONAL Question:\n",
    "\n",
    "- Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "https://en.wikipedia.org/wiki/Power_law\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.4  Synonym detection over 2Gig of Data\n",
    "\n",
    "For the remainder of this assignment you will work with two datasets:\n",
    "\n",
    "### 1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    "DocA {X:20, Y:30, Z:5}\n",
    "DocB {X:100, Y:20}\n",
    "DocC {M:5, N:20, Z:5}\n",
    "\n",
    "\n",
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 - 5.5.1 Please unit test and system test your code with respect to SYSTEMS TEST DATASET and show the results. Please compute the expected answer by hand and show your hand calculations for the SYSTEMS TEST DATASET. Then show the results you get with you system.\n",
    "\n",
    "In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. To accomplish this you must script two main tasks using MRJob:\n",
    "\n",
    "**Task 1:** Build stripes for the most frequent 10,000 words using cooccurence information based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "** Task 2:** Using two (symmetric) comparison methods of your choice (e.g., correlations, distances, similarities), pairwise compare all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "** Design notes for (1)**    \n",
    "For this task you will be able to modify the pattern we used in HW 3.2 (feel free to use the solution as reference). To total the word counts across the 5-grams, output the support from the mappers using the total order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts, the mapper must also output co-occurrence counts for the pairs of words inside of each 5-gram. Treat these words as a basket,as we have in HW 3, but count all stripes or pairs in both orders, i.e., count both orderings: (word1,word2), and (word2,word1), to preserve symmetry in our output for (2).\n",
    "\n",
    "** Design notes for (2)**    \n",
    "For this task you will have to determine a method of comparison. Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "...\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to parallelize than others, and do not perform more associations than is necessary, since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.4 - Build Stripe Docs for Word Co-Occurences\n",
    "\n",
    "We are using the vocabulary based on the words with frequency ranked 9001-10,000 taken from the \"Top Ten Words\" problem above that generated a text file of the top 10,000 words (toptenwords.txt).  The vocabulary file is \"bottomwords.txt\" (words 9001-10000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripes.py\n",
    "#HW 5.4 - Stripes MRJob Definition\n",
    "from __future__ import division\n",
    "from itertools import combinations\n",
    "import csv\n",
    "\n",
    "from mrjob import conf\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class BuildStripes(MRJob):\n",
    "    \n",
    "    def jobconf(self):\n",
    "        orig_jobconf = super(BuildStripes, self).jobconf()\n",
    "        # Setting these high enough improves EMR job speed\n",
    "        custom_jobconf = {\n",
    "            \"mapred.map.tasks\":28,\n",
    "            \"mapred.reduce.tasks\":28\n",
    "        }\n",
    "        return conf.combine_dicts(orig_jobconf, custom_jobconf)\n",
    "    \n",
    "\n",
    "    def mapper_init(self):\n",
    "        \"\"\"Load file for vocabulary -> words with frequency 9001-10000\"\"\"\n",
    "        self.word_dict={}\n",
    "        \n",
    "        #This is the file of words with frequency ranked 9000-10000 that we\n",
    "        with open('bottomwords.txt','rb') as f:\n",
    "            for row in f.readlines():\n",
    "                line=row.strip().split('\\t')\n",
    "                self.word_dict[line[0][1:-1]]=line[1]\n",
    "        \n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"\n",
    "        Emit co-occurrence combinations for each pair of relevant words stored as stripes\n",
    "        \"\"\"\n",
    "        line=line.strip().split('\\t') # parse ngram line into tokens\n",
    "        ngram=line[0].lower() # convert all tokens to lowercase\n",
    "        count=int(line[1]) # count of occurrences in corpus\n",
    "        ngram_words=ngram.split(\" \") # individual words in our ngram\n",
    "        output={}\n",
    "        \n",
    "        #Pull out words from ngram that we care about (those that appear in the corpus we loaded earlier)\n",
    "        words=[i for i in ngram_words if i in self.word_dict.keys()]\n",
    "        \n",
    "        # Update output stripe for each combination of co-occurring, relevant words\n",
    "        for word1,word2 in combinations(words,2):\n",
    "            \n",
    "            if word1 in output.keys():\n",
    "                output[word1][word2]=output[word1].get(word2,0)+count\n",
    "            else:\n",
    "                output[word1]={word2:count}\n",
    "            \n",
    "            # maintain symmetry for opposite co-occurence\n",
    "            if word2 in output.keys():      \n",
    "                output[word2][word1]=output[word2].get(word1,0)+count\n",
    "            else:\n",
    "                output[word2]={word1:count}\n",
    "        \n",
    "        # generate output of word, co-occurrence\n",
    "        for word,cooccur in output.iteritems():\n",
    "            yield word,cooccur\n",
    "            \n",
    "    def reducer(self,word,cos):\n",
    "        \"\"\"Aggregate stripes based on intermediate results from mapper\"\"\"\n",
    "        output_dict={}\n",
    "        for co in cos:\n",
    "            # The second_word variable here is so named to distinguish it from the \"word\" in the input\n",
    "            # and refers to the words in the co-occurrence stripe\n",
    "            for second_word,count in co.iteritems():\n",
    "                output_dict[second_word] = output_dict.get(second_word,0)+count\n",
    "        yield word, output_dict\n",
    "        \n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper\n",
    "                #We can recycle the reducer as combiner here, which is nice\n",
    "                ,combiner=self.reducer\n",
    "                ,reducer=self.reducer\n",
    "                  )        \n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    BuildStripes.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary (Most Frequent Words 9,001-10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"surveys\"\t169333\r\n",
      "\"jungle\"\t169314\r\n",
      "\"lacked\"\t169282\r\n",
      "\"correlate\"\t169273\r\n",
      "\"boxes\"\t169237\r\n",
      "\"escort\"\t169220\r\n",
      "\"disclosed\"\t169132\r\n",
      "\"shepherd\"\t169114\r\n",
      "\"commend\"\t169081\r\n",
      "\"zenith\"\t169049\r\n",
      "\"multiplication\"\t169025\r\n",
      "\"epic\"\t169004\r\n",
      "\"literacy\"\t168967\r\n",
      "\"atonement\"\t168908\r\n",
      "\"soda\"\t168897\r\n"
     ]
    }
   ],
   "source": [
    "!cat bottomwords.txt | head -15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\r\n",
      "A Biography of General George\t92\t90\t74\r\n",
      "A Case Study in Government\t102\t102\t78\r\n",
      "A Case Study of Female\t447\t447\t327\r\n",
      "A Case Study of Limited\t55\t55\t43\r\n",
      "A Child's Christmas in Wales\t1099\t1061\t866\r\n",
      "A Circumstantial Narrative of the\t62\t62\t50\r\n",
      "A City by the Sea\t62\t60\t49\r\n",
      "A Collection of Fairy Tales\t123\t117\t80\r\n",
      "A Collection of Forms of\t116\t103\t82\r\n",
      "A Commentary on his Apology\t110\t110\t69\r\n",
      "A Comparative Study of Juvenile\t68\t64\t44\r\n",
      "A Comparison of the Properties\t72\t72\t60\r\n",
      "A Conceptual Framework and the\t91\t91\t67\r\n",
      "A Conceptual Framework for Life\t49\t49\t40\r\n",
      "A Concise Bibliography of the\t145\t143\t122\r\n",
      "A Continuation of the Letters\t52\t51\t40\r\n",
      "A Critical Review and a\t197\t194\t155\r\n",
      "A Critique and a Guide\t42\t42\t42\r\n",
      "A Defence of the Royal\t153\t153\t120"
     ]
    }
   ],
   "source": [
    "!cat ngramtest.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing small_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile small_test.txt\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.4 Driver - Local Test for Creating Stripes\n",
    "\n",
    "This creates stripes using the vocabulary of 1000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from stripes import BuildStripes\n",
    "\n",
    "def run_5_4_stripe_test():\n",
    "    mr_job = BuildStripes(args=['ngramtest.txt','--file','bottomwords.txt'])\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            print mr_job.parse_output_line(line)\n",
    "\n",
    "run_5_4_stripe_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Test Stripes on Amazon EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/jamesgray/.mrjob.conf\n",
      "Using s3://mrjob-5a4b4386e2160458/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/ld/9wpyxfw13t7_pdv_0b8958x40000gn/T/stripes.jamesgray.20160619.143900.403013\n",
      "Copying local files to s3://mrjob-5a4b4386e2160458/tmp/stripes.jamesgray.20160619.143900.403013/files/...\n",
      "Adding our job to existing cluster j-1MTWMS2Y5VS2R\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.4.0:\n",
      "The have been translated as follows\n",
      " mapred.map.tasks: mapreduce.job.maps\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "Waiting for step 1 of 1 (s-61PMO8YM2A1S) to complete...\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40176/cluster\n",
      "  RUNNING for 22.9s\n",
      "     5.0% complete\n",
      "  RUNNING for 55.9s\n",
      "    39.1% complete\n",
      "  RUNNING for 87.4s\n",
      "    82.1% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-61PMO8YM2A1S on ec2-54-84-230-119.compute-1.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-84-230-119.compute-1.amazonaws.com/mnt/var/log/hadoop/steps/s-61PMO8YM2A1S/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=11078\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=560\n",
      "\t\tFILE: Number of bytes written=5976323\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2465\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=29\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=11078\n",
      "\t\tS3: Number of bytes written=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=29\n",
      "\t\tLaunched map tasks=29\n",
      "\t\tLaunched reduce tasks=28\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=931648320\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1032454080\n",
      "\t\tTotal time spent by all map tasks (ms)=646978\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=29114010\n",
      "\t\tTotal time spent by all reduce tasks (ms)=358491\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=32264190\n",
      "\t\tTotal vcore-seconds taken by all map tasks=646978\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=358491\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=52510\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=13785\n",
      "\t\tInput split bytes=2465\n",
      "\t\tMap input records=20\n",
      "\t\tMap output bytes=0\n",
      "\t\tMap output materialized bytes=12992\n",
      "\t\tMap output records=0\n",
      "\t\tMerged Map outputs=812\n",
      "\t\tPhysical memory (bytes) snapshot=22207410176\n",
      "\t\tReduce input groups=0\n",
      "\t\tReduce input records=0\n",
      "\t\tReduce output records=0\n",
      "\t\tReduce shuffle bytes=12992\n",
      "\t\tShuffled Maps =812\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=25874661376\n",
      "\t\tVirtual memory (bytes) snapshot=147800072192\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-5a4b4386e2160458/tmp/stripes.jamesgray.20160619.143900.403013/...\n",
      "Removing temp directory /var/folders/ld/9wpyxfw13t7_pdv_0b8958x40000gn/T/stripes.jamesgray.20160619.143900.403013...\n",
      "Killing our SSH tunnel (pid 32789)\n"
     ]
    }
   ],
   "source": [
    "!python ./stripes.py \\\n",
    "    -r emr s3://jamesgray-w261/ngramtest.txt  \\\n",
    "    --file ./bottomwords.txt \\\n",
    "    --cluster-id j-1MTWMS2Y5VS2R \\\n",
    "    --output-dir=s3://jamesgray-w261/stripes_unittest \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Data Set Stripes on Amazon EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/jamesgray/.mrjob.conf\n",
      "Using s3://mrjob-5a4b4386e2160458/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/ld/9wpyxfw13t7_pdv_0b8958x40000gn/T/stripes.jamesgray.20160619.144912.668222\n",
      "Copying local files to s3://mrjob-5a4b4386e2160458/tmp/stripes.jamesgray.20160619.144912.668222/files/...\n",
      "Adding our job to existing cluster j-1MTWMS2Y5VS2R\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.4.0:\n",
      "The have been translated as follows\n",
      " mapred.map.tasks: mapreduce.job.maps\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "Waiting for step 1 of 1 (s-1O9UEVM1FB654) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40176/cluster\n",
      "  RUNNING for 21.1s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 53.7s\n",
      "  RUNNING for 84.8s\n",
      "  RUNNING for 116.4s\n",
      "  RUNNING for 147.1s\n",
      "  RUNNING for 200.1s\n",
      "  RUNNING for 341.1s\n",
      "  RUNNING for 372.3s\n",
      "  RUNNING for 414.0s\n",
      "  RUNNING for 444.3s\n",
      "  RUNNING for 476.0s\n",
      "  RUNNING for 506.7s\n",
      "  RUNNING for 537.6s\n",
      "  RUNNING for 568.6s\n",
      "  RUNNING for 857.1s\n",
      "  RUNNING for 887.5s\n",
      "  RUNNING for 919.0s\n",
      "  RUNNING for 949.3s\n",
      "  RUNNING for 980.2s\n",
      "  RUNNING for 1010.7s\n",
      "  RUNNING for 1042.2s\n",
      "  RUNNING for 1072.5s\n",
      "  RUNNING for 1104.0s\n",
      "  RUNNING for 1134.7s\n",
      "  RUNNING for 1166.1s\n",
      "  RUNNING for 1197.1s\n",
      "  RUNNING for 1228.7s\n",
      "  RUNNING for 1259.2s\n",
      "  RUNNING for 1290.4s\n",
      "  RUNNING for 1321.6s\n",
      "  RUNNING for 1352.7s\n",
      "  RUNNING for 1383.7s\n",
      "  RUNNING for 1414.9s\n",
      "  RUNNING for 1445.4s\n",
      "  RUNNING for 1476.3s\n",
      "  RUNNING for 1507.1s\n",
      "  RUNNING for 1537.8s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-1O9UEVM1FB654 on ec2-54-84-230-119.compute-1.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-84-230-119.compute-1.amazonaws.com/mnt/var/log/hadoop/steps/s-1O9UEVM1FB654/syslog.2016-06-19-14\n",
      "  Parsing step log: ssh://ec2-54-84-230-119.compute-1.amazonaws.com/mnt/var/log/hadoop/steps/s-1O9UEVM1FB654/syslog\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=214733\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=297513\n",
      "\t\tFILE: Number of bytes written=23939371\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=23450\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=190\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=2156069116\n",
      "\t\tS3: Number of bytes written=214733\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=188\n",
      "\t\tLaunched map tasks=190\n",
      "\t\tLaunched reduce tasks=28\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=32833229760\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=17170038720\n",
      "\t\tTotal time spent by all map tasks (ms)=22800854\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1026038430\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5961819\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=536563710\n",
      "\t\tTotal vcore-seconds taken by all map tasks=22800854\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5961819\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=12520850\n",
      "\t\tCombine input records=26439\n",
      "\t\tCombine output records=23092\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=64209\n",
      "\t\tInput split bytes=23450\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=725208\n",
      "\t\tMap output materialized bytes=784186\n",
      "\t\tMap output records=26439\n",
      "\t\tMerged Map outputs=5320\n",
      "\t\tPhysical memory (bytes) snapshot=114795364352\n",
      "\t\tReduce input groups=997\n",
      "\t\tReduce input records=23092\n",
      "\t\tReduce output records=997\n",
      "\t\tReduce shuffle bytes=784186\n",
      "\t\tShuffled Maps =5320\n",
      "\t\tSpilled Records=46184\n",
      "\t\tTotal committed heap usage (bytes)=131261267968\n",
      "\t\tVirtual memory (bytes) snapshot=464755920896\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-5a4b4386e2160458/tmp/stripes.jamesgray.20160619.144912.668222/...\n",
      "Removing temp directory /var/folders/ld/9wpyxfw13t7_pdv_0b8958x40000gn/T/stripes.jamesgray.20160619.144912.668222...\n",
      "Killing our SSH tunnel (pid 32828)\n"
     ]
    }
   ],
   "source": [
    "!python ./stripes.py \\\n",
    "    -r emr s3://filtered-5grams  \\\n",
    "    --file ./bottomwords.txt \\\n",
    "    --cluster-id j-1MTWMS2Y5VS2R \\\n",
    "    --output-dir=s3://jamesgray-w261/stripes_full \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.4 - Calculate Word Similarity for Unit Test Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing simunit.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile simunit.txt\n",
    "(DocA {X:20, Y:30, Z:5})\n",
    "(DocB {X:100, Y:20}) \n",
    "(DocC {M:5, N:20, Z:5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.4 - Calculate Word Similarity Using Inverted Index and Cosine Simularity method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.4 - Calculate Word Similarity Using Inverted Index and Jaccard Simularity method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.5 Evaluation of synonyms that your discovered\n",
    "\n",
    "In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4).\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined\n",
    "by your measure in HW5.4, and use the synonyms function in the accompanying\n",
    "python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.6 Optional\n",
    "\n",
    "Repeat HW5 using vocabulary words ranked from 8001,-10,000;  7001,-10,000; 6001,-10,000; 5001,-10,000; 3001,-10,000; and 1001,-10,000;\n",
    "\n",
    "Dont forget to report you Cluster configuration.\n",
    "\n",
    "Generate the following graphs:\n",
    "-- vocabulary size (X-Axis) versus CPU time for indexing\n",
    "-- vocabulary size (X-Axis) versus number of pairs processed\n",
    "-- vocabulary size (X-Axis) versus F1 measure, Precision, Recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.7 (optional)\n",
    "Once again, benchmark your top 10,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
